# 📝 하이브리드 리트리버 구축 작업 로그

**작성 일시:** 2025-11-22 01:00 (KST)  
**최종 업데이트:** 2025-11-22 04:00 (KST)  
**작성자:** RAG 개발 담당자  
**관련 단계:** 2.5. 하이브리드 리트리버 구축 (Hybrid Retrieval)

---

## 1. 누가, 언제, 어디서 (Who, When, Where)

### 1.1 누가 (Who)
*   보험 약관 RAG 시스템의 검색 성능을 향상시키기 위해 하이브리드 리트리버를 구축하려는 엔지니어.

### 1.2 언제 (When)
*   Phase 2 (임베딩 및 벡터 DB 구축) 완료 직후.
*   Dense Retrieval만으로는 부족하다는 한계를 인식하고, 키워드 검색(BM25)을 추가하여 검색 정확도를 높이려는 시점.

### 1.3 어디서 (Where)
*   작업 경로: `/home/pencilfoxs/0_Insurance_PF/03_Retrieval`
*   실험 스크립트: `compare_tokenizers.py`, `debug_tokenizers.py`
*   평가 데이터: `02_Embedding/evaluation_dataset.json` (30개 FAQ)

---

## 2. 가설 (Hypothesis) & 초기 가정

### 2.1 문제 인식 (Problem Statement)
*   **현재 상태:** Phase 2에서 **Dense Retrieval (의미 기반 벡터 검색)**만 구현 완료.
*   **발견된 한계:** 보험 약관은 "음주운전" 같은 의미 검색도 중요하지만, **"무보험차상해"**, **"제3조"**, **"자기부담금"** 같은 **정확한 키워드(용어) 매칭**이 매우 중요함.
*   **가설:** Dense 단독보다 **BM25(키워드 검색)를 결합한 하이브리드(Hybrid) 리트리버**가 보험 약관 검색 성능을 크게 향상시킬 것이다.

### 2.2 초기 가정
1.  **형태소 분석기 선정:** 한국어 복합명사 처리 능력이 뛰어난 **Kiwi**가 보험 약관 도메인에서 가장 우수할 것으로 예상.
2.  **하이브리드 효과:** Dense와 Sparse를 결합하면 Recall@5가 단독 사용 대비 최소 10% 이상 향상될 것으로 예상.

---

## 3. 무엇을, 어떻게 (What, How)

### 3.1 무엇을 (What)
*   **하이브리드 리트리버 구축:** Dense(의미 기반)와 Sparse(키워드 기반) 검색을 결합한 리트리버 구현.
*   **최적 형태소 분석기 선정:** BM25 검색에 사용할 한국어 토크나이저 비교 및 선정.

## 4. 실험 설계 (Experiment Design)

### 4.1 비교 대상 후보군 (Candidates)

#### A. 형태소 분석기 (Tokenizer for BM25)
1.  **Kiwi (키위)**
    *   **선정 배경:** 최신 알고리즘 기반으로 복합명사 분해 능력이 뛰어남. "자동차손해배상보장법" 같은 긴 법률 용어를 정확히 처리할 것으로 기대.
    *   **특징:** 사용자 사전 추가 용이, 속도 빠름.
2.  **Mecab (메캅)**
    *   **선정 배경:** 전통적으로 한국어 NLP에서 속도와 안정성 1위로 평가됨. 대량 데이터 처리에 검증된 성능.
    *   **특징:** 설치 복잡도 높음, 하지만 성숙한 라이브러리.
3.  **Okt (Open Korean Text)**
    *   **선정 배경:** 대중적 인지도가 높고, 정규화 기능이 있어 약관의 다양한 표현을 통일할 수 있을 것으로 기대.
    *   **특징:** 인터넷 용어/구어체에 강하나, 문어체(약관) 성능은 검증 필요.

#### B. 검색 방식 (Retrieval Methods)
1.  **Only Dense:** 현재 구현된 `ko-sroberta` + ChromaDB (Baseline)
2.  **Only Sparse:** 선정된 최적 토크나이저 + BM25
3.  **Hybrid (Ensemble):** Dense(0.5) + Sparse(0.5) 가중치 결합

### 4.2 테스트 데이터셋 (Test Dataset)
*   **데이터셋:** `02_Embedding/evaluation_dataset.json` (30개 FAQ)
    *   **구성:** 보상(8), 면책(7), 특약(5), 절차(5), 정의(5) 등 5개 카테고리.
    *   **특징:** 각 질문마다 정답(Positive Passage)과 오답(Hard Negative)이 포함되어 있어, 리트리버의 변별력을 측정하기에 적합.

### 4.3 평가 지표 (Metrics)
리트리버의 핵심 미덕은 **"정답을 놓치지 않고 후보군에 포함시키는 것"**이므로, 다음 지표를 사용함.

1.  **Recall@5 (재현율):** 검색 결과 상위 5개 안에 정답 문서가 포함될 확률. (가장 중요)
2.  **MRR@5 (Mean Reciprocal Rank):** 정답이 몇 번째 순위에 등장하는지 (1등이면 1.0, 2등이면 0.5...).
3.  **Execution Time:** 검색 소요 시간 (실시간 서비스 고려).

### 4.4 실험 절차 (Procedure)
1.  **Step 1: 토크나이저 챔피언십**
    *   Kiwi vs Mecab vs Okt를 사용하여 30개 질문에 대한 BM25 검색 수행.
    *   `Recall@5`가 가장 높은 토크나이저를 **우승자**로 선정.
2.  **Step 2: 하이브리드 효용성 검증**
    *   (우승한 BM25)와 (기존 벡터 DB)를 결합.
    *   **Dense 단독** vs **BM25 단독** vs **Hybrid** 성능 비교.
3.  **Step 3: 최종 리트리버 확정**
    *   검증된 조합으로 RAG 파이프라인 업그레이드.

---

## 5. 검증 결과 (Validation)

### 5.1 Step 1: 토크나이저 비교 실험 결과 (Tokenizer Championship)

터미널 실행(`compare_tokenizers.py`) 결과:

| 순위 | 토크나이저 | Recall@5 | MRR@5 | Index Time | 비고 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1위** | **Mecab** | **1.0000** | **0.9833** | **19.29s** | **최고 성능 + 최고 속도** |
| 2위 | Kiwi | 1.0000 | 0.9667 | 122.95s | MRR은 우수하나 속도가 느림 |
| 3위 | Okt | 1.0000 | 0.9389 | 120.47s | 모든 지표에서 가장 낮음 |

#### 3.1.1 결과 분석
*   **Mecab 압승:** 
    *   **MRR@5 0.9833**으로 가장 높음 → 정답 문서를 가장 높은 순위에 배치.
    *   **Index Time 19.29초**로 Kiwi(122.95초) 대비 **6배 이상 빠름**.
    *   이전 프로젝트(`History_Docent_PJ_gemini`)에서 사용한 `python-mecab-ko` 라이브러리로 성공적으로 설치 및 실행됨.
*   **초기 가설 기각:** "Kiwi가 복합명사 처리에 강하다"는 가설이 틀렸음. 실제로는 **Mecab이 보험 약관 도메인에서 가장 우수한 성능**을 보임.

### 5.1.2 정성 평가 결과 (Qualitative Evaluation)
*   **평가 방법:** `debug_tokenizers.py`를 통해 4개 카테고리(법률용어, 복합명사, 조건/숫자, 구어체) 총 **20개 샘플 문장**을 각 토크나이저로 분석.
*   **평가 범위:**
    *   법률/전문 용어: 5개 (예: "자동차손해배상보장법", "기명피보험자")
    *   복합 명사 및 띄어쓰기: 5개 (예: "무보험자동차상해담보", "비상급유서비스")
    *   조건 및 숫자: 5개 (예: "제15조 제2항", "0.03퍼센트", "20만원")
    *   애매모호한 표현/구어체: 5개 (예: "문콕했는데", "어떡해요")
*   **주요 발견사항:**
    *   **Mecab 강점:** 
        *   복합명사 처리 일관성 최우수 ("무보험자동차상해담보" → "무보험 | 자동차 | 상해 | 담보")
        *   전문 용어 처리 안정적 ("기명피보험자" → "기명 | 피보험자")
    *   **Mecab 약점:** 
        *   숫자 분해 문제 ("0.03" → "0 | . | 03"), 다만 실제 검색 성능에는 영향 미미 (정량 평가에서 증명)
        *   구어체 약함 ("사고 냈는데" → "사 | 고 | 냈")
    *   **Kiwi 강점:** 긴 복합명사 보존 능력 ("자동차손해배상보장법" 하나로 유지)
    *   **Kiwi 약점:** 일부 복합명사 과도 분해 ("비상급유서비스" → "비 | 상급 | 유 | 서비스")
    *   **Okt 강점:** 구어체/금액 표현 자연스러움 ("20만원" 하나로 유지)
    *   **Okt 약점:** 전문 용어 처리 취약 ("기명피보험자" → "기 | 명피 | 보험 | 자")
*   **종합 판단:** 정량 평가(MRR 0.9833)와 정성 평가를 종합하여, 보험 약관 도메인(문어체, 전문 용어 중심)에서는 **Mecab이 가장 적합**함을 확인.
*   **상세 분석 리포트:** `qualitative_analysis.md` 참조

### 5.2 Step 2: BM25 리트리버 구축 (완료 ✅)

*   **구현 파일:** `bm25_retriever.py`
*   **기능:**
    *   `MecabBM25Retriever` 클래스: LangChain `BaseRetriever` 인터페이스 준수
    *   피클링 기능: 인덱스 저장/로드 (`bm25_index.pkl`)
    *   문서 로드: `chunked_data.jsonl`에서 6,402개 문서 로드 및 인덱싱
*   **결과:** BM25 인덱스 구축 완료 (소요시간 약 20초)

### 5.3 Step 3: 하이브리드 리트리버 통합 (완료 ✅)

*   **구현 파일:** `hybrid_retriever.py`
*   **기능:**
    *   `HybridRetriever` 클래스: Dense + Sparse 결합
    *   **RRF (Reciprocal Rank Fusion)** 알고리즘 직접 구현
    *   각 리트리버에서 Top-50 추출 후 RRF로 결합하여 최종 Top-K 반환
*   **결과:** 하이브리드 리트리버 정상 작동 확인

### 5.4 Step 4: 하이브리드 효용성 검증 (완료 ✅)

터미널 실행(`evaluate_hybrid.py`) 결과:

| 리트리버 | Recall@5 | MRR@5 | Avg Time | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **Sparse Only** | **1.0000** | **0.9833** | **0.0246s** | **최고 성능** |
| **Hybrid** | **1.0000** | 0.9444 | 0.0431s | Recall 동일, MRR 약간 낮음 |
| Dense Only | 0.7000 | 0.5483 | 0.0474s | 가장 낮은 성능 |

#### 5.4.1 결과 분석
*   **예상과 다른 결과:**
    *   초기 가설("Hybrid이 가장 좋을 것")과 달리, **Sparse Only가 모든 지표에서 최고 성능**을 기록.
    *   Hybrid는 Recall@5는 1.0000을 달성했으나, MRR@5(0.9444)가 Sparse(0.9833)보다 낮음.
*   **Dense Only의 한계:**
    *   Recall@5 0.7000으로 30개 중 9개를 놓침. 보험 약관의 정확한 키워드 매칭이 매우 중요함을 증명.
*   **Win/Loss 분석:**
    *   Hybrid vs Dense: 15승 1패 14무 → Hybrid가 Dense보다 대체로 우수.
    *   하지만 Sparse가 이미 충분히 좋아서 Hybrid의 추가 효과가 제한적.
*   **결론:**
    *   보험 약관 도메인에서는 **키워드 검색(BM25)이 의미 검색(Dense)보다 훨씬 효과적**임.
    *   Hybrid는 Dense의 약점을 보완하지만, Sparse가 이미 충분히 강력하여 추가 이점이 제한적.

---

## 6. 왜 (Why) - 의사결정 (Conclusion & Pivot)

### 6.1 Step 1 의사결정: 토크나이저 선정 (Pivot)

*   **초기 가설:** Kiwi가 최고일 것으로 예상.
*   **실험 결과:** **Mecab이 모든 지표에서 압승.**
*   **최종 선정:** **Mecab (python-mecab-ko)**
*   **선정 이유:**
    1.  **정량 평가 우수:** MRR@5 0.9833으로 최고 성능. 정답 문서를 가장 높은 순위에 배치하여 검색 품질이 최우수.
    2.  **정성 평가 우수:** 20개 샘플 분석 결과, 복합명사 처리 일관성이 가장 뛰어남. 보험 약관의 전문 용어를 안정적으로 처리.
    3.  **속도 압도적:** Index Time 19.29초로 Kiwi(122.95초) 대비 6배 이상 빠른 인덱싱 속도로 실시간 서비스에 유리.
    4.  **안정성:** 이전 프로젝트(`History_Docent_PJ_gemini`)에서 검증된 `python-mecab-ko` 라이브러리 사용으로 신뢰성 확보.

### 6.2 최종 의사결정: 리트리버 전략

*   **검증 결과:** Sparse Only (Mecab + BM25)가 모든 지표에서 최고 성능.
*   **최종 선택:**
    *   **프로덕션 리트리버:** **Sparse Only (Mecab + BM25)** 사용.
    *   **이유:** Recall@5 1.0000, MRR@5 0.9833으로 압도적 성능. 속도도 가장 빠름(0.0246s).
*   **Hybrid의 역할:**
    *   Dense의 약점을 보완하지만, Sparse가 이미 충분히 강력하여 추가 이점이 제한적.
    *   향후 Dense 모델 개선 시 재검토 가능.

---

### 5.5 Step 5: 리트리버 문맥 품질 검증 (완료 ✅)

*   **구현 파일:** `test_retriever_context.py`
*   **목적:** LLM에게 전달될 검색 결과가 실제로 답변 생성에 충분한 정보를 담고 있는지 육안으로 확인
*   **테스트 쿼리:** 5개 (면책/보상, 수치/조건, 절차, 정의, 복합 상황)
*   **결과:**
    *   **성공:** 3/5 (60%) - 명확한 키워드 질문에서 우수한 성능
    *   **실패:** 2/5 (40%) - 동의어 처리 및 복합 질문에 한계
*   **상세 분석:** `retriever_context_analysis.md` 참조

### 5.6 Step 6: 리트리버 정밀 진단 (50개 질문) (완료 ✅)

*   **구현 파일:** `diagnose_failures.py`
*   **목적:** 대규모 테스트를 통해 리트리버의 약점을 정밀 분석하고 실패 케이스를 자동 분류
*   **테스트 데이터:** 총 50개 (기존 30개 FAQ + 추가 20개)
    *   카테고리: 동의어(5), 복합(5), 수치(5), 절차(5), 면책(7), 보상(8), 정의(5), 특약(5)
*   **결과:**
    *   **성공률: 96.0% (48/50)** - 매우 우수한 성능
    *   **실패: 2개**
        *   Q47: "가지급금 받을 수 있나요?" (절차 카테고리)
        *   Q50: "주소 변경하려면 어떻게?" (절차 카테고리)
*   **카테고리별 성능:**
    *   **동의어:** 5/5 (100%) ✅
    *   **복합:** 5/5 (100%) ✅
    *   **수치:** 5/5 (100%) ✅
    *   **절차:** 3/5 (60%) ⚠️ **약점**
    *   **면책:** 7/7 (100%) ✅
    *   **보상:** 8/8 (100%) ✅
    *   **정의:** 5/5 (100%) ✅
    *   **특약:** 5/5 (100%) ✅
*   **실패 원인 분석:**
    *   **공통점:** 모두 "절차" 카테고리에서 발생
    *   **원인:** 사용자 질문의 키워드("가지급금", "주소 변경")와 약관의 실제 표현("가지급금", "알릴 의무") 간 불일치
    *   **해결 방안:** 절차 관련 동의어 사전 구축 필요
*   **상세 리포트:** `failure_analysis_report.md` 참조

### 5.7 Step 7: Dense 리트리버 재평가 및 최종 비교 (완료 ✅)

*   **목적:** Dense + 벡터 DB 접근 방식을 다시 시험하여 Sparse 대비 성능/속도 우위를 확인.
*   **실험 구성:**
    1.  **모델 후보 비교(`compare_models_dense.py`, 50문항):**
        | 모델 | Avg Pos | Avg Sep | Recall | Time/item |
        | --- | --- | --- | --- | --- |
        | `jhgan/ko-sroberta-multitask` | 0.4907 | **0.1640** | **0.7600** | 0.044s |
        | `BAAI/bge-m3` | 0.6362 | 0.0728 | 0.7400 | 0.059s |
        | `intfloat/multilingual-e5-large` | **0.8463** | 0.0356 | 0.6800 | 0.059s |
        → Separation/Recall/속도 균형이 가장 좋은 `ko-sroberta` 유지 결정.
    2.  **Chroma 벡터 DB 재구축(`create_vector_db.py`)**  
        *   GPU A100 사용, 6,402 청크 재인덱싱 (43.6초)
    3.  **Dense 전용 평가(`evaluate_dense.py`, 50문항):**  
        *   **Recall@5 0.9600**, **MRR@5 0.8200**, **0.035s/query**
    4.  **Dense vs Sparse vs Hybrid 비교(`evaluate_hybrid.py`, 50문항):**
        | 리트리버 | Recall@5 | MRR@5 | Avg Time | 비고 |
        | --- | --- | --- | --- | --- |
        | Dense Only (`ko-sroberta`) | 0.9600 | 0.8200 | 0.0346s | 의미 검색 |
        | **Sparse Only (Mecab+BM25)** | **1.0000** | **0.9700** | **0.0190s** | **최고 성능/최고 속도** |
        | Hybrid (Dense+Sparse) | 1.0000 | 0.9667 | 0.0376s | Dense 보완, 속도 손해 |
*   **결론:** Dense 재구축으로 성능이 개선되었으나, 여전히 Sparse 단독보다 낮은 MRR/속도. Hybrid도 Sparse를 뛰어넘지는 못함. 따라서 **프로덕션은 Sparse Only 유지, Dense/Hyrbid는 보조 연구용으로 보관.**

### 5.8 Step 8: Cross-Encoder 리랭커 실험 (완료 ✅)

*   **목적:** BM25가 반환한 Top-10 문서를 Cross-Encoder로 재정렬해 LLM이 읽는 상위 문서 품질을 개선.
*   **후보 모델:** `cross-encoder/ms-marco-MiniLM-L-6-v2` (소형, 빠른 추론 속도)
*   **실험 구성 (`evaluate_reranker.py`, 50문항):**
    | 시나리오 | Recall@5 | MRR@5 | Avg Time |
    | --- | --- | --- | --- |
    | BM25 Top-5 (Baseline) | 1.0000 | 0.9700 | 0.0207s |
    | BM25 + Cross-Encoder Rerank Top-5 | 1.0000 | 0.9700 | 0.0592s (BM25+Rerank) |
*   **결과:** Recall/MRR 변화 없음. Cross-Encoder 추가 시간(~+0.038s/query)만 발생.
*   **분석:** 이미 BM25가 정답을 1~2위에 배치하고 있어 리랭커 이득이 거의 없음. 절차형 실패 케이스도 개선되지 않았음.
*   **결론:** 현 단계에서는 리랭커 미도입 유지. 추후 Dense 기반 재진입 시 재검토.

## 6. 최종 결론 및 개선 방안

### 6.1 리트리버 성능 종합 평가

*   **정량 평가 (30개 FAQ):** Recall@5 1.0000, MRR@5 0.9833
*   **정성 평가 (5개 샘플):** 60% 성공률 (소규모 테스트)
*   **정밀 진단 (50개 질문):** **96.0% 성공률** - 매우 우수한 성능 확인

### 6.2 발견된 약점 및 개선 방안

#### 약점 1: 절차 관련 질문의 키워드 불일치
*   **증거:** 50개 테스트 중 실패 2개 모두 "절차" 카테고리
*   **원인:** 사용자 질문 표현과 약관 용어 간 차이
*   **개선 방안:**
    1.  **단기:** 절차 관련 동의어 사전 구축 및 질문 전처리
    2.  **중기:** 리랭커 도입으로 의미적 관련성 재평가

#### 약점 2: (미미함)
*   현재 96% 성공률로 대부분의 케이스를 잘 처리하고 있어, 추가 개선의 우선순위는 낮음.

### 6.3 최종 의사결정

*   **프로덕션 리트리버:** **Sparse Only (Mecab + BM25)** 유지
*   **이유:** 96% 성공률로 충분히 우수하며, 절차 관련 2개 실패 케이스는 동의어 확장으로 해결 가능
*   **리랭커 도입:** 현재는 선택사항. 향후 성능 개선이 필요할 때 재검토

---

**Next Step:** Phase 3 (LLM 연동 및 답변 생성) 준비. 선정된 Sparse 리트리버를 사용하여 RAG 파이프라인 구축.

