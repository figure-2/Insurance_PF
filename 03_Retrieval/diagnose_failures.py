"""
ë¦¬íŠ¸ë¦¬ë²„ ì •ë°€ ì§„ë‹¨ ë° ì‹¤íŒ¨ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (50ê°œ ì§ˆë¬¸)
ì˜¤ë‹µ ë…¸íŠ¸ ìë™ ìƒì„± ê¸°ëŠ¥ í¬í•¨
"""

import json
import os
import sys
import time
from typing import List, Dict, Any
from mecab import MeCab as MeCabKo

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from bm25_retriever import MecabBM25Retriever

# -----------------------------------------------------------------------------
# ì¶”ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„° (ê¸°ì¡´ 30ê°œ + ì¶”ê°€ 20ê°œ = ì´ 50ê°œ)
# -----------------------------------------------------------------------------
ADDITIONAL_QUERIES = [
    # ë™ì˜ì–´/ìœ ì‚¬ì–´ í…ŒìŠ¤íŠ¸
    {"query": "ëº‘ì†Œë‹ˆ ì‚¬ê³  ë³´ìƒë˜ë‚˜ìš”?", "positive": "ë³´ìœ ë¶ˆëª…ìë™ì°¨ì— ì˜í•œ ì‚¬ê³ ", "category": "ë™ì˜ì–´"},
    {"query": "ìì°¨ë¶€ë‹´ê¸ˆ ì–¼ë§ˆì˜ˆìš”?", "positive": "ìê¸°ì°¨ëŸ‰ì†í•´ ìê¸°ë¶€ë‹´ê¸ˆ", "category": "ë™ì˜ì–´"},
    {"query": "ëŒ€ë¦¬ê¸°ì‚¬ ì‚¬ê³  ë³´ìƒ", "positive": "ëŒ€ë¦¬ìš´ì „ìê°€ ìš´ì „ ì¤‘ ì‚¬ê³ ", "category": "ë™ì˜ì–´"},
    {"query": "ê²¬ì¸ ê±°ë¦¬ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "positive": "ê¸´ê¸‰ê²¬ì¸ì„œë¹„ìŠ¤", "category": "ë™ì˜ì–´"},
    {"query": "ë°°í„°ë¦¬ ë°©ì „ëì–´ìš”", "positive": "ë°°í„°ë¦¬ì¶©ì „ì„œë¹„ìŠ¤", "category": "ë™ì˜ì–´"},
    
    # ë³µí•© ìƒí™© í…ŒìŠ¤íŠ¸
    {"query": "ì—¬í–‰ ì¤‘ ë Œí„°ì¹´ ë¹Œë ¸ëŠ”ë° ë‚´ ë³´í—˜ ë˜ë‚˜ìš”?", "positive": "ë‹¤ë¥¸ìë™ì°¨ìš´ì „ë‹´ë³´íŠ¹ì•½", "category": "ë³µí•©"},
    {"query": "ê°€ì¡±ì´ ë‚´ ì°¨ ëª°ë‹¤ê°€ ì‚¬ê³ ë‚˜ë©´?", "positive": "ìš´ì „ì ë²”ìœ„ ë° ì—°ë ¹ í•œì •", "category": "ë³µí•©"},
    {"query": "ì°¨ìœ ë¦¬ ëŒ ë§ì•„ì„œ ê¹¨ì¡ŒëŠ”ë° ë³´ìƒë˜ë‚˜ìš”?", "positive": "ìê¸°ì°¨ëŸ‰ì†í•´", "category": "ë³µí•©"},
    {"query": "íƒœí’ìœ¼ë¡œ ì¹¨ìˆ˜ë˜ë©´ ë³´ìƒë˜ë‚˜ìš”?", "positive": "ìê¸°ì°¨ëŸ‰ì†í•´ ë³´ìƒí•˜ëŠ” ì†í•´", "category": "ë³µí•©"},
    {"query": "ë¬¸ì½• ë‹¹í–ˆëŠ”ë° ìƒëŒ€ë°©ì„ ëª» ì°¾ìœ¼ë©´?", "positive": "ë¬¼ì ì‚¬ê³  í• ì¦ê¸°ì¤€", "category": "ë³µí•©"},
    
    # êµ¬ì²´ì  ìˆ˜ì¹˜/ì¡°ê±´
    {"query": "ìŒì£¼ìš´ì „ ë¶€ë‹´ê¸ˆ ì–¼ë§ˆ?", "positive": "ìŒì£¼ìš´ì „ ì‚¬ê³ ë¶€ë‹´ê¸ˆ", "category": "ìˆ˜ì¹˜"},
    {"query": "ë¬´ë©´í—ˆìš´ì „ ë¶€ë‹´ê¸ˆ", "positive": "ë¬´ë©´í—ˆìš´ì „ ì‚¬ê³ ë¶€ë‹´ê¸ˆ", "category": "ìˆ˜ì¹˜"},
    {"query": "í• ì¦ ê¸°ì¤€ ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?", "positive": "ë¬¼ì ì‚¬ê³  í• ì¦ê¸°ì¤€ê¸ˆì•¡", "category": "ìˆ˜ì¹˜"},
    {"query": "ê¸´ê¸‰ì¶œë™ ëª‡ ë²ˆ ë¶€ë¥¼ ìˆ˜ ìˆë‚˜ìš”?", "positive": "ì—°ê°„ ì´ìš©í•œë„", "category": "ìˆ˜ì¹˜"},
    {"query": "ëŒ€ë¬¼ë°°ìƒ ìµœì†Œ ê°€ì…ê¸ˆì•¡", "positive": "ëŒ€ë¬¼ë°°ìƒ ì˜ë¬´ë³´í—˜ ê°€ì…ê¸ˆì•¡", "category": "ìˆ˜ì¹˜"},
    
    # ì ˆì°¨/ì„œë¥˜
    {"query": "ë³´í—˜ê¸ˆ ì²­êµ¬ ì„œë¥˜ ë­ í•„ìš”í•´?", "positive": "ë³´í—˜ê¸ˆ ì²­êµ¬ì‹œ êµ¬ë¹„ì„œë¥˜", "category": "ì ˆì°¨"},
    {"query": "ê°€ì§€ê¸‰ê¸ˆ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?", "positive": "ê°€ì§€ê¸‰ê¸ˆ", "category": "ì ˆì°¨"},
    {"query": "ë³´í—˜ë£Œ ë¶„í•  ë‚©ë¶€ ë˜ë‚˜ìš”?", "positive": "ë³´í—˜ë£Œì˜ ë¶„í• ë‚©ì…", "category": "ì ˆì°¨"},
    {"query": "ê³„ì•½ ì·¨ì†Œí•˜ê³  ì‹¶ì€ë°ìš”", "positive": "ê³„ì•½ì˜ ì·¨ì†Œ", "category": "ì ˆì°¨"},
    {"query": "ì£¼ì†Œ ë³€ê²½í•˜ë ¤ë©´ ì–´ë–»ê²Œ?", "positive": "ì•Œë¦´ ì˜ë¬´", "category": "ì ˆì°¨"},
]

def load_dataset():
    """ê¸°ì¡´ ë°ì´í„°ì…‹ + ì¶”ê°€ ë°ì´í„°ì…‹ ë³‘í•©"""
    base_path = "/home/pencilfoxs/0_Insurance_PF/02_Embedding/evaluation_dataset.json"
    with open(base_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    # ì¶”ê°€ ë°ì´í„°ì…‹ ë³‘í•©
    for item in ADDITIONAL_QUERIES:
        dataset.append({
            "query": item["query"],
            "positive": item["positive"],
            "category": item.get("category", "ê¸°íƒ€"),
            "type": "additional"
        })
    
    return dataset

def find_rank(retriever, query, positive_text, tokenizer):
    """ì •ë‹µ ë¬¸ì„œì˜ ìˆœìœ„ ì°¾ê¸°"""
    results = retriever.invoke(query)
    positive_tokens = set(tokenizer.morphs(positive_text))
    
    for rank, doc in enumerate(results, 1):
        # 1. ë‹¨ìˆœ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if positive_text in doc.page_content:
            return rank, doc
            
        # 2. í† í° ê²¹ì¹¨ ë¹„ìœ¨ í™•ì¸ (30% ì´ìƒ)
        doc_tokens = set(tokenizer.morphs(doc.page_content))
        overlap = len(positive_tokens & doc_tokens)
        if len(positive_tokens) > 0 and (overlap / len(positive_tokens)) >= 0.3:
            return rank, doc
            
    return 999, None

def analyze_failure(query, positive, top_doc):
    """ì‹¤íŒ¨ ì›ì¸ ìë™ ë¶„ì„ (ê°„ì´)"""
    if not top_doc:
        return "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        
    return f"Top-1 ë¬¸ì„œ: {top_doc.metadata.get('breadcrumbs', 'N/A')} (ê´€ë ¨ì„± ë‚®ìŒ)"

def main():
    print("ğŸš€ ë¦¬íŠ¸ë¦¬ë²„ ì •ë°€ ì§„ë‹¨ ì‹œì‘ (ì´ 50ê°œ ì§ˆë¬¸)")
    print("="*60)
    
    # ë¦¬íŠ¸ë¦¬ë²„ ë¡œë“œ
    index_path = "/home/pencilfoxs/0_Insurance_PF/03_Retrieval/bm25_index.pkl"
    try:
        retriever = MecabBM25Retriever.load_index(index_path)
        retriever.k = 5  # Top-5 ê²€ì‚¬
    except Exception as e:
        print(f"Error loading index: {e}")
        return

    tokenizer = MeCabKo()
    dataset = load_dataset()
    
    results = []
    failures = []
    
    for i, item in enumerate(dataset, 1):
        query = item['query']
        positive = item['positive']
        category = item.get('category', 'General')
        
        rank, found_doc = find_rank(retriever, query, positive, tokenizer)
        
        result = {
            "id": i,
            "query": query,
            "positive": positive,
            "rank": rank,
            "category": category
        }
        results.append(result)
        
        if rank > 5:  # Top-5 ì§„ì… ì‹¤íŒ¨
            # ì‹¤íŒ¨ ì‹œ Top-1 ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¶„ì„ìš©)
            top_results = retriever.invoke(query)
            top_doc = top_results[0] if top_results else None
            analysis = analyze_failure(query, positive, top_doc)
            
            failures.append({
                **result,
                "analysis": analysis,
                "top_1_content": top_doc.page_content[:100] if top_doc else ""
            })
            print(f"âŒ [Fail] Q{i}: {query} (Rank: {rank})")
        else:
            print(f"âœ… [Pass] Q{i}: {query} (Rank: {rank})")

    # í†µê³„ ê³„ì‚°
    success_count = len(dataset) - len(failures)
    success_rate = (success_count / len(dataset)) * 100
    
    print("\n" + "="*60)
    print(f"ğŸ“Š ì§„ë‹¨ ê²°ê³¼ ìš”ì•½ (ì´ {len(dataset)}ê°œ)")
    print(f"   - ì„±ê³µ: {success_count}ê°œ ({success_rate:.1f}%)")
    print(f"   - ì‹¤íŒ¨: {len(failures)}ê°œ")
    print("="*60)
    
    # ë¦¬í¬íŠ¸ ì‘ì„±
    report_path = "/home/pencilfoxs/0_Insurance_PF/03_Retrieval/failure_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ©º ë¦¬íŠ¸ë¦¬ë²„ ì‹¤íŒ¨ ë¶„ì„ ë¦¬í¬íŠ¸ (Failure Analysis)\n\n")
        f.write(f"**ì‘ì„± ì¼ì‹œ:** {time.strftime('%Y-%m-%d %H:%M')}\n")
        f.write(f"**í…ŒìŠ¤íŠ¸ ë°ì´í„°:** ì´ {len(dataset)}ê°œ (ê¸°ì¡´ 30 + ì¶”ê°€ 20)\n")
        f.write(f"**ì„±ê³µë¥ :** {success_rate:.1f}% ({success_count}/{len(dataset)})\n\n")
        
        f.write("## 1. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëª©ë¡ (Top-5 ì§„ì… ì‹¤íŒ¨)\n")
        for fail in failures:
            f.write(f"### âŒ Q{fail['id']}. {fail['query']}\n")
            f.write(f"- **ì¹´í…Œê³ ë¦¬:** {fail['category']}\n")
            f.write(f"- **ì •ë‹µ í‚¤ì›Œë“œ:** `{fail['positive']}`\n")
            f.write(f"- **ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:** {fail['analysis']}\n")
            f.write(f"- **Top-1 ë¬¸ì„œ ë‚´ìš©:** {fail['top_1_content']}...\n\n")
            
        f.write("## 2. ì¹´í…Œê³ ë¦¬ë³„ ì‹¤íŒ¨ìœ¨\n")
        cats = set(r['category'] for r in results)
        for cat in cats:
            total = sum(1 for r in results if r['category'] == cat)
            failed = sum(1 for f in failures if f['category'] == cat)
            rate = (failed / total) * 100 if total > 0 else 0
            f.write(f"- **{cat}:** {total}ê°œ ì¤‘ {failed}ê°œ ì‹¤íŒ¨ ({rate:.1f}%)\n")

    print(f"\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")

if __name__ == "__main__":
    main()

