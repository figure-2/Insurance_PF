"""
í˜•íƒœì†Œ ë¶„ì„ê¸° ë¹„êµ ì‹¤í—˜: Kiwi vs Mecab vs Okt
BM25 ê²€ìƒ‰ ì„±ëŠ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì  í† í¬ë‚˜ì´ì € ì„ ì •
"""

import json
import time
from typing import List, Dict, Tuple
from rank_bm25 import BM25Okapi
import numpy as np

# í˜•íƒœì†Œ ë¶„ì„ê¸° import
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    print("Warning: Kiwi not available")

try:
    from mecab import MeCab as MeCabKo
    MECAB_KO_AVAILABLE = True
except ImportError:
    MECAB_KO_AVAILABLE = False
    print("Warning: python-mecab-ko not available")

try:
    from konlpy.tag import Okt
    KONLPY_OKT_AVAILABLE = True
except ImportError:
    KONLPY_OKT_AVAILABLE = False
    print("Warning: Konlpy Okt not available")


class TokenizerWrapper:
    """í˜•íƒœì†Œ ë¶„ì„ê¸° ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, tokenizer):
        self.name = name
        self.tokenizer = tokenizer
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if self.name == "Kiwi":
            # KiwiëŠ” analyze() ë©”ì„œë“œ ì‚¬ìš© (ê²°ê³¼: [(í˜•íƒœì†Œ, í’ˆì‚¬, ì‹œì‘ìœ„ì¹˜, ëìœ„ì¹˜), ...])
            try:
                result = self.tokenizer.analyze(text)
                if result and len(result) > 0 and len(result[0]) > 0:
                    return [morph for morph, pos, _, _ in result[0][0]]
                return []
            except:
                return []
        elif self.name == "Mecab":
            # python-mecab-koëŠ” morphs() ë©”ì„œë“œ ì‚¬ìš©
            return self.tokenizer.morphs(text)
        elif self.name == "Okt":
            # OktëŠ” morphs() ë©”ì„œë“œ ì‚¬ìš©
            return self.tokenizer.morphs(text)
        else:
            raise ValueError(f"Unknown tokenizer: {self.name}")


def load_documents(jsonl_path: str) -> List[Dict]:
    """chunked_data.jsonl ë¡œë“œ"""
    documents = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            documents.append({
                'chunk_id': item['chunk_id'],
                'text': item['text'],
                'metadata': item['metadata']
            })
    return documents


def load_evaluation_dataset(json_path: str) -> List[Dict]:
    """evaluation_dataset.json ë¡œë“œ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def calculate_recall_at_k(results: List[int], k: int = 5) -> float:
    """Recall@K ê³„ì‚°: ì •ë‹µì´ ìƒìœ„ Kê°œ ì•ˆì— í¬í•¨ë˜ëŠ”ì§€"""
    if not results:
        return 0.0
    return sum(1 for rank in results if rank <= k) / len(results)


def calculate_mrr_at_k(results: List[int], k: int = 5) -> float:
    """MRR@K ê³„ì‚°: ì •ë‹µì˜ ì—­ìˆœìœ„ í‰ê· """
    if not results:
        return 0.0
    reciprocal_ranks = []
    for rank in results:
        if rank <= k:
            reciprocal_ranks.append(1.0 / rank)
        else:
            reciprocal_ranks.append(0.0)
    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0


def find_positive_in_documents(query: str, positive: str, documents: List[Dict], 
                               tokenizer: TokenizerWrapper, bm25: BM25Okapi, 
                               top_k: int = 50) -> int:
    """
    ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ positive passageê°€ í¬í•¨ëœ ë¬¸ì„œì˜ ìˆœìœ„ë¥¼ ì°¾ìŒ
    Returns: ìˆœìœ„ (1-based), ì—†ìœ¼ë©´ 999
    """
    # ì§ˆë¬¸ í† í¬ë‚˜ì´ì§•
    query_tokens = tokenizer.tokenize(query)
    
    # BM25 ê²€ìƒ‰
    scores = bm25.get_scores(query_tokens)
    top_indices = np.argsort(scores)[::-1][:top_k]
    
    # positive passageì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•: positiveë¥¼ í† í¬ë‚˜ì´ì§•)
    positive_tokens = set(tokenizer.tokenize(positive))
    
    # ìƒìœ„ Kê°œ ë¬¸ì„œ ì¤‘ positive í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë¬¸ì„œ ì°¾ê¸°
    best_match_rank = None
    for rank, idx in enumerate(top_indices, start=1):
        doc_text = documents[idx]['text']
        doc_tokens = set(tokenizer.tokenize(doc_text))
        
        # í‚¤ì›Œë“œ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°
        overlap = len(positive_tokens & doc_tokens)
        if overlap >= len(positive_tokens) * 0.3:  # 30% ì´ìƒ ê²¹ì¹˜ë©´ ì •ë‹µìœ¼ë¡œ ê°„ì£¼
            best_match_rank = rank
            break
    
    return best_match_rank if best_match_rank else 999


def evaluate_tokenizer(tokenizer: TokenizerWrapper, documents: List[Dict], 
                       eval_dataset: List[Dict]) -> Dict:
    """íŠ¹ì • í† í¬ë‚˜ì´ì €ë¡œ BM25 ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""
    print(f"\n{'='*60}")
    print(f"Testing Tokenizer: {tokenizer.name}")
    print(f"{'='*60}")
    
    # 1. ë¬¸ì„œ í† í¬ë‚˜ì´ì§• ë° BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
    print("Building BM25 index...")
    start_time = time.time()
    
    tokenized_docs = []
    for doc in documents:
        tokens = tokenizer.tokenize(doc['text'])
        tokenized_docs.append(tokens)
    
    bm25 = BM25Okapi(tokenized_docs)
    index_time = time.time() - start_time
    print(f"Index built in {index_time:.2f}s")
    
    # 2. í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("Running retrieval tests...")
    ranks = []
    
    for i, item in enumerate(eval_dataset):
        query = item['query']
        positive = item['positive']
        
        rank = find_positive_in_documents(query, positive, documents, tokenizer, bm25)
        ranks.append(rank)
        
        if (i + 1) % 10 == 0:
            print(f"  Processed {i+1}/{len(eval_dataset)} queries...")
    
    # 3. ë©”íŠ¸ë¦­ ê³„ì‚°
    recall_at_5 = calculate_recall_at_k(ranks, k=5)
    mrr_at_5 = calculate_mrr_at_k(ranks, k=5)
    
    # 4. ê²°ê³¼ ë°˜í™˜
    return {
        'tokenizer': tokenizer.name,
        'index_time': index_time,
        'recall_at_5': recall_at_5,
        'mrr_at_5': mrr_at_5,
        'ranks': ranks
    }


def main():
    # ê²½ë¡œ ì„¤ì •
    DATA_PATH = "/home/pencilfoxs/0_Insurance_PF/01_Preprocessing/chunked_data.jsonl"
    EVAL_PATH = "/home/pencilfoxs/0_Insurance_PF/02_Embedding/evaluation_dataset.json"
    
    print("Loading documents and evaluation dataset...")
    documents = load_documents(DATA_PATH)
    eval_dataset = load_evaluation_dataset(EVAL_PATH)
    print(f"Loaded {len(documents)} documents and {len(eval_dataset)} test queries")
    
    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizers = []
    
    if KIWI_AVAILABLE:
        try:
            kiwi = Kiwi()
            tokenizers.append(TokenizerWrapper("Kiwi", kiwi))
        except Exception as e:
            print(f"Failed to initialize Kiwi: {e}")
    
    if MECAB_KO_AVAILABLE:
        try:
            mecab = MeCabKo()
            tokenizers.append(TokenizerWrapper("Mecab", mecab))
        except Exception as e:
            print(f"Failed to initialize Mecab: {e}")
    
    if KONLPY_OKT_AVAILABLE:
        try:
            okt = Okt()
            tokenizers.append(TokenizerWrapper("Okt", okt))
        except Exception as e:
            print(f"Failed to initialize Okt: {e}")
    
    if not tokenizers:
        print("ERROR: No tokenizers available!")
        return
    
    # ê° í† í¬ë‚˜ì´ì € í‰ê°€
    results = []
    for tokenizer in tokenizers:
        try:
            result = evaluate_tokenizer(tokenizer, documents, eval_dataset)
            results.append(result)
        except Exception as e:
            print(f"Error evaluating {tokenizer.name}: {e}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("FINAL RESULTS")
    print("="*60)
    print(f"{'Tokenizer':<15} | {'Recall@5':<10} | {'MRR@5':<10} | {'Index Time':<12}")
    print("-" * 60)
    
    for result in results:
        print(f"{result['tokenizer']:<15} | {result['recall_at_5']:.4f}     | "
              f"{result['mrr_at_5']:.4f}     | {result['index_time']:.2f}s")
    
    # ìµœê³  ì„±ëŠ¥ í† í¬ë‚˜ì´ì € ì„ ì • (MRR@5 ìš°ì„ , ë™ì¼í•˜ë©´ Recall@5)
    if results:
        best = max(results, key=lambda x: (x['mrr_at_5'], x['recall_at_5']))
        print(f"\nğŸ† Best Tokenizer: {best['tokenizer']}")
        print(f"   - Recall@5: {best['recall_at_5']:.4f}")
        print(f"   - MRR@5: {best['mrr_at_5']:.4f}")
        print(f"   - Index Time: {best['index_time']:.2f}s")


if __name__ == "__main__":
    main()

