# 📝 LLM 연동 및 베이스라인 구축 로그

**작성 일시:** 2025-11-22 04:10 (KST)  
**최종 업데이트:** 2025-11-22 05:50 (KST)  
**작성자:** RAG 개발 담당자  
**관련 단계:** 3.0 LLM 연동 및 베이스라인 구축

---

## 1. 누가, 언제, 어디서 (Who, When, Where)

### 1.1 누가 (Who)
*   보험 약관 RAG 시스템을 완성하기 위해 LLM을 도입하고자 하는 엔지니어.

### 1.2 언제 (When)
*   리트리버(Phase 2)가 안정화되고, Sparse Only 전략이 확정된 이후 단계.

### 1.3 어디서 (Where)
*   작업 경로: `/home/pencilfoxs/0_Insurance_PF/04_LLM`
*   주요 스크립트: `rag_baseline.py`, `evaluate_llm_responses.py` (신규 작성 예정)
*   데이터: `evaluation_dataset.json` (확장 50문항), 향후 QA 파인튜닝용 자체 데이터셋

---

## 2. 가설 (Hypothesis) & 초기 가정

### 2.1 문제 인식
*   **현재 상태:** 리트리버는 96% 성공률로 정답 문맥을 안정적으로 찾아줌.
*   **문제:** 소/중형 LLM은 보험 도메인 지식 부족으로 정확한 답변을 못 하거나 참조를 생략함.
*   **가설:** Sparse 리트리버 + 경량 LLM 조합으로도 일정 수준의 정답률을 확보할 수 있으며, 부족한 부분은 파인튜닝으로 개선 가능하다.

### 2.2 초기 가정
1.  **모델 후보:** `Llama-2-13B`, `Mistral-7B`, `SOLAR-10.7B`, (한국어 특화) `KULLM 12.8B`  
2.  **RAG 구성:** Sparse 리트리버 Top-3 문서를 LLM Prompt에 삽입.
3.  **평가 지표:** 정답률(정량), 출처 인용 여부, 답변 완결성(정성).

---

## 3. 실험 설계 (Experiment Design)

### 3.1 단계별 계획
1.  **Baseline RAG (파인튜닝 전):**
    *   각 LLM 후보에 대해 동일 Prompt Template으로 RAG 실행.
    *   20~30개 대표 질문(보상/면책/절차/정의)으로 정확도 측정.
2.  **LLM 평가 스크립트:**
    *   `rag_baseline.py`: 질의 → 리트리버 → LLM → 답변 출력.
    *   `evaluate_llm_responses.py`: 정량/정성 평가 결과 CSV/Markdown 기록.
3.  **결과 기록:**
    *   Rule 5에 맞춰 후보 선정, 실험 규모, 정량/정성 지표, 실패 분석, 최종 결론을 문서화.

### 3.2 평가 지표 (Metrics)
1.  **Accuracy:** 정답 기준과 일치 여부 (Partial Credit 포함).
2.  **Reference Usage:** 답변 내 명시적 출처 언급 여부.
3.  **Response Quality:** 전문가 검토(약관 준수 여부, 위험 안내 포함 여부).
4.  **Latency:** 응답 생성 시간(추후 서비스화 고려).

---

## 4. 다음 액션

1.  LLM 후보별 환경 준비 (HF Transformers + 4bit LoRA 추후 적용 가능하도록 세팅)
2.  Baseline RAG 스크립트 작성 및 1차 평가
3.  Baseline 결과 기록 후, 파인튜닝 데이터 설계 단계로 이동

---

## 5. 베이스라인 드라이런 (2025-11-22 04:15)

*   **스크립트:** `rag_baseline.py` (Top-3 BM25 Context + LLM)
*   **모델:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (오픈 접근 가능)
*   **질문:** "음주운전하면 보상 되나요?"
*   **관찰:**  
    *   리트리버는 음주운전 사고부담금 표가 포함된 적절한 문맥을 제공.  
    *   LLM은 "보상 없다"라고 단정하여 사실과 다르게 응답 → 도메인 지식 부족 확인.  
*   **다음 조치:**  
    1. 여러 LLM 후보(7B~13B)까지 동일 스크립트로 베이스라인 측정  
    2. 평가 지표/포맷(`evaluate_llm_responses.py`) 마련  
    3. 파인튜닝/LoRA 계획 수립

---

**Next Step:** `rag_baseline.py`로 LLM 후보군(7B~13B) 베이스라인 측정 + 평가 스크립트 준비.

---

## 6. 베이스라인 모델 교체 및 1차 평가 (Step 1) - 2025-11-22 05:50

**규칙 5번 (비교 실험 공통 규범)에 의거하여 모델 교체 실험 결과를 기록한다.**

### 6.1 후보 선정 사유 (Why Candidates)
*   **기존 모델:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
    *   **선정 이유:** 오픈 접근 가능, 경량 모델로 빠른 테스트 가능
    *   **실패 원인:** 한국어 토큰 처리 불가능, Mojibake(깨진 문자열) 발생
*   **변경 모델:** `beomi/Llama-3-Open-Ko-8B`
    *   **선정 이유:**
        1. 현재 한국어 오픈소스 LLM 중 표준(Standard)으로 평가받음
        2. 8B 파라미터는 단일 GPU(T4/L4)에서 4-bit 양자화로 구동 가능한 마지노선
        3. Llama-3 아키텍처 기반으로 Instruction Tuning 가능
    *   **기술 설정:** 4-bit Quantization (`bitsandbytes`), `nf4` 타입, `double_quant=True`

### 6.2 실험 규모 (Experiment Scale)
*   **데이터셋:** 단일 쿼리 ("음주운전하면 면책인가요?")
*   **리트리버:** BM25 Sparse Retriever (Top-3 문서)
*   **평가 방식:** 정성 평가 (Qualitative) - 답변 품질 육안 검증
*   **실행 환경:** GCP 인스턴스, CUDA 사용

### 6.3 정량 평가 (Quantitative Evaluation)
*   **모델 로드 시간:** 약 2분 20초 (정상 구동 확인)
*   **Inference 시간:** 약 3-5초 (정상 범위)
*   **한글 출력:** 정상화됨 (Mojibake 해결 확인)
*   **GPU 메모리 사용량:** 약 5-6GB (4-bit 양자화 효과 확인)

### 6.4 정성 평가 (Qualitative Evaluation)
*   **테스트 쿼리:** "음주운전하면 면책인가요?"
*   **검색된 문맥 (Context):**
    *   문서[1]: AXA손해보험 - 12대 중과실 설명 (음주운전 포함)
    *   문서[2]: 캐롯손해보험 - 동일증권 설명 (관련성 낮음)
    *   문서[3]: AXA손해보험 - 동일증권 및 물적할증 설명 (관련성 낮음)
*   **LLM 출력:**
    ```
    음주운전 면책은 없습니다.
    이번에 새로 나온 신약이요.
    新薬は今回新たに
    今回新しく出た新薬です。
    ...
    (이후 포브스 기사, 김정숙 여사 관련 텍스트 생성)
    ```
*   **평가 결과:** **FAIL (실패)**
    *   질문에 대한 답변이 아니라, 학습 데이터에 있던 뉴스 기사 등을 무작위로 생성
    *   Hallucination(환각) 및 Continuation(문서 이어쓰기) 현상 발생
    *   Instruction(지시)을 따르지 않고, 프롬프트를 "문서의 일부"로 인식

### 6.5 실패 분석 (Failure Analysis)
*   **근본 원인:**
    1. **Base Model의 한계:** `beomi/Llama-3-Open-Ko-8B`는 **Base Model(기반 모델)**로, Instruction Tuning(지시 이행 학습)이 되어 있지 않음
    2. **학습 목표 불일치:** 이 모델은 "질문에 답하는 법"을 배운 적이 없고, 단순히 "주어진 텍스트 뒤에 이어질 말을 예측"하도록 학습됨
    3. **프롬프트 해석 오류:** 프롬프트를 "명령"으로 인식하지 못하고 "문서의 일부"로 인식하여 뒷 내용을 작문함
*   **약한 케이스:**
    *   모든 Instruction 기반 질문에 대해 동일한 문제 발생 예상
    *   도메인 특화 지식(보험 약관)이 아닌 일반 학습 데이터를 기반으로 응답 생성
*   **교훈:**
    *   Base Model을 그대로 RAG에 사용하는 것은 불가능
    *   도메인 데이터(약관)와 지시(Instruction) 데이터로 **파인튜닝(Fine-tuning)**하는 과정이 필수적

### 6.6 최종 결론 및 의사결정 (Conclusion & Pivot)
*   **인프라 확정:**
    1. 4-bit 양자화 로딩 환경 구축 완료
    2. 한글 입출력 정상화 확인
    3. `rag_baseline_v2.py` 스크립트 완성
*   **전략 유지 (Pivot 없음):**
    *   Base Model의 한계를 확인했으므로, 예정대로 **Step 3 (데이터 생성) 및 Step 4 (QLoRA 파인튜닝)**을 진행
    *   목표: "지시를 잘 따르는 보험 전문가 모델" 구축
*   **포트폴리오 강조점:**
    *   이 "실패"는 역설적으로 **파인튜닝 프로젝트의 당위성**을 완벽하게 증명
    *   *"베이스 모델은 횡설수설하지만, 파인튜닝 후에는 전문가처럼 답한다"*는 스토리 완성

### 6.7 다음 단계
*   **Step 2:** 베이스라인 정밀 평가 (다양한 질문으로 재테스트) - 선택적
*   **Step 3:** 파인튜닝용 데이터셋 생성 (필수)
*   **Step 4:** QLoRA 파인튜닝 수행 (필수)

