# 📝 Base Model vs Fine-Tuned Model 비교 평가 로그

**작성 일시:** 2025-11-24 23:45 (KST)  
**최종 업데이트:** 2025-11-24 13:20 (KST)  
**작성자:** RAG 개발 담당자  
**관련 단계:** 6.0 모델 비교 평가 및 최종 채택, 6.1 RAG 통합 및 3개 모델 비교

---

## 1. 누가, 언제, 어디서 (Who, When, Where)

### 1.1 누가 (Who)
*   보험 약관 RAG 시스템의 LLM 성능을 검증하기 위해 Base Model과 Fine-Tuned Model을 비교 평가하는 엔지니어.

### 1.2 언제 (When)
*   Fine-Tuned Model (v3) 학습 완료 후, 학습하지 않은 데이터(Test Set)에 대한 일반화 성능을 검증하기 위해 수행.
*   RAG 시스템 통합 전, 모델 단독 성능을 최종 확인하는 단계.

### 1.3 어디서 (Where)
*   작업 경로: `/home/pencilfoxs/0_Insurance_PF/06_Evaluation`
*   평가 스크립트: `05_FineTuning/compare_base_vs_finetuned.py`
*   결과 파일: `05_FineTuning/comparison_base_vs_finetuned.json`
*   테스트 데이터: `05_FineTuning/test_20.json` (전체 데이터의 20%, 학습에 사용되지 않음)

---

## 2. 가설 (Hypothesis) & 초기 가정

### 2.1 문제 인식 (Problem Statement)
*   **현재 상태:** Fine-Tuned Model (v3)이 학습 데이터(Train Set)에 대해서는 높은 성능을 보였으나, **학습하지 않은 새로운 질문(Test Set)에 대해서도 동일한 성능을 보이는지 검증이 필요함.**
*   **문제:** 모델이 단순히 학습 데이터를 암기한 것인지, 아니면 보험 도메인 지식을 일반화하여 학습한 것인지 확인 필요.
*   **가설:** Fine-Tuned Model은 Base Model 대비 **"약관 스타일의 답변 생성"**, **"구체적인 예시 제공"**, **"약관 조항 인용"** 능력이 향상되어 있을 것이다.

### 2.2 초기 가정
1.  **Base Model:** 범용 모델이므로 보험 약관에 대한 전문 지식이 부족하고, 일반적인 답변을 생성할 것으로 예상.
2.  **Fine-Tuned Model:** RAG 스타일 데이터셋으로 학습했으므로, 약관 내용을 참고하여 답변하는 능력이 향상되었을 것으로 예상.
3.  **한계:** 모델 단독 추론만으로는 약관 조항 번호를 정확히 맞추기 어려울 수 있음 (RAG 통합 필요).

---

## 3. 실험 설계 (Experiment Design)

### 3.1 후보 선정 사유 (Why Candidates)

#### A. Base Model (Candidate A)
*   **모델:** `beomi/Llama-3-Open-Ko-8B`
*   **선정 이유:**
    1.  Fine-Tuned Model의 성능 향상 폭을 측정하기 위한 **기준점(Baseline)**으로 선정.
    2.  파인튜닝 전후의 차이를 명확히 보여주기 위해, Fine-Tuned Model과 동일한 베이스 모델 사용.

#### B. Fine-Tuned Model (Candidate B)
*   **모델:** `llama-3-ko-insurance-lora-v3` (8:2 Split 학습)
*   **선정 이유:**
    1.  학습 데이터에 포함되지 않은 **Unseen Data(Test Set)**에 대한 일반화 성능을 검증하기 위함.
    2.  v2(전체 학습)와 달리, v3는 검증용 데이터셋으로 분리하여 학습했으므로, Test Set 성능이 더 신뢰할 수 있음.

### 3.2 실험 규모 (Experiment Scale)
*   **데이터셋:** `test_20.json` (전체 데이터의 20%, 1,088개 중 학습에 사용되지 않은 평가용 데이터)
*   **샘플 수:** 전체 1,088개 중 **무작위 50개 샘플** 추출 (Fast Check)
*   **질문 유형 분포:**
    *   Easy: 일상적 질문 (예: "이게 뭐예요?")
    *   Fact: 사실 확인 질문 (예: "정확히 몇 일인가요?")
    *   Scenario: 상황 기반 질문 (예: "제가 이런 상황인데 보상받을 수 있나요?")

### 3.3 평가 기준 (Criteria)
1.  **정확성 (Accuracy):** 약관 내용을 왜곡하지 않고 사실대로 전달하는가?
2.  **구체성 (Specificity):** 약관 조항 번호나 구체적인 수치를 명시하는가?
3.  **친절함 (Tone):** 고객 응대 톤앤매너(CS 스타일)를 유지하는가?
4.  **예시 제공 (Example):** 복잡한 개념을 이해하기 쉽게 예시를 들어 설명하는가?

---

## 4. 검증 결과 (Validation)

### 4.1 정량 평가 (Quantitative Evaluation)

**실행 스크립트:** `05_FineTuning/compare_base_vs_finetuned.py`  
**결과 파일:** `05_FineTuning/comparison_base_vs_finetuned.json`

*   **평가 샘플 수:** 50개
*   **평가 완료 시간:** 2025-11-24 (백그라운드 실행 완료)

> **참고:** 정량 평가 지표(Accuracy, BLEU 등)는 추후 Ragas 라이브러리를 활용하여 추가 측정 예정.

### 4.2 정성 평가 (Qualitative Evaluation) - 20개 이상 샘플 분석

`comparison_base_vs_finetuned.json` 파일에서 대표적인 성공/실패 사례를 발췌하여 분석합니다.

#### **4.2.1 성공 사례 (Success Cases)**

| 유형 | 질문 요약 | Base Model 답변 | Fine-Tuned Model 답변 | 평가 |
| :--- | :--- | :--- | :--- | :--- |
| **Scenario** | 자기신체사고 2배 보장 (후유장애 7급, 3천만원 가입 시) | "약관에 따라 보상 처리될 수 있습니다. 1,200만원을 기준으로 합니다." (단순 사실 나열) | "**최대 6천만원까지 보상을 받으실 수 있습니다.** 원래 보험가입금액 3천만원의 두 배인..." (구체적 계산 및 예시 제공) | **Fine-Tuned 승** (고객 맞춤형 구체적 수치 제시) |
| **Scenario** | 무보험차 상해 (타 보험 중복 시) | "중복해서 돈을 받지는 못하고..." (일반적인 설명) | "나머지 부분에 대해서만 우리가 돈을 더 드려요. **예를 들어 100만원 먼저 받았다면...**" (예시를 통한 설명) | **Fine-Tuned 승** (이해하기 쉬운 예시 추가) |
| **Scenario** | 옆집 도자기 파손 (자녀 사고) | "약관 3항에 해당할 수 있습니다. 보상받기 어렵습니다." | "약관 3항 '피보험자와 생계를 같이하는 배우자 및 직계친족'에 해당하여 보상을 기대하기 어렵습니다." (근거 조항 명시) | **Fine-Tuned 승** (약관 조항 인용 능력 우수) |
| **Easy** | 청약 철회 시 보험료 반환 기간 | "3영업일 이내" | "3영업일 이내" | **무승부** (단순 사실 확인 질문에는 둘 다 정확함) |
| **Fact** | 동승자 과실 비율 (+10% vs +20% 기준) | "명확하게 구분하여... 내용이 명시되어 있지 않습니다." | "구체적인 조건이나 판단 기준이 명시되어 있지 않습니다." (유사함) | **무승부** (둘 다 약관의 불명확함을 정확히 지적) |

#### **4.2.2 실패 사례 (Failure Cases)**

| 유형 | 질문 요약 | Base Model 답변 | Fine-Tuned Model 답변 | 문제점 |
| :--- | :--- | :--- | :--- | :--- |
| **Scenario** | 잠금장치 해제 서비스 제외 | "잠금장치를 해제하는 것이 필요합니다." (오답) | "잠금장치를 해제해야 운행할 수 있는 상태입니다." (오답) | **공통 실패** (약관에 "제외"라고 명시되어 있으나, 두 모델 모두 이를 놓침) |
| **Scenario** | 아내가 운전한 차에 치이는 사고 | "보상받으실 가능성이 높습니다." (오답) | "보상받으실 가능성이 높습니다." (오답) | **공통 실패** (약관에 "배우자에 의한 사고는 보상하지 않음" 명시되어 있으나, 두 모델 모두 이를 놓침) |

### 4.3 실패 분석 (Failure Analysis)

#### **4.3.1 공통적인 한계**
1.  **약관 조항 번호 정확도 부족:**
    *   두 모델 모두 **"약관 제 몇 조"**인지를 정확히 맞추지 못하는 경우가 간혹 발생함.
    *   **원인:** LLM 파인튜닝만으로는 방대한 약관의 모든 조항 번호를 완벽히 외우기 어려움.
    *   **해결책:** 이는 **RAG 시스템과 결합**하여, 검색된 문맥(Context)을 프롬프트에 넣어줌으로써 해결해야 함.

2.  **부정 표현 처리 어려움:**
    *   "보상하지 않는다", "제외한다" 같은 부정 표현을 놓치는 경우가 있음.
    *   **원인:** 모델이 긍정적 답변에 편향되어 학습되었을 가능성.
    *   **해결책:** 학습 데이터에 부정 사례를 더 많이 포함하거나, RAG 검색 결과를 더 정확히 활용.

#### **4.3.2 Fine-Tuned Model의 개선점**
*   **설명력 향상:** Base Model 대비 답변이 구체적이며, 예시를 들어 설명하는 능력이 탁월함.
*   **톤앤매너:** 전문적이면서도 친절한 상담원 말투를 잘 학습함.
*   **약관 스타일:** "약관 제X조에 따르면..." 같은 표현을 더 자주 사용함.

---

## 5. 의사결정 (Conclusion & Pivot)

### 5.1 최종 결론 (Final Conclusion)

#### **채택 모델:** **Fine-Tuned Model (v3)**

#### **채택 근거:**
1.  **설명력 향상:** Base Model 대비 답변이 구체적이며, 예시를 들어 설명하는 능력이 탁월함.
2.  **톤앤매너:** 전문적이면서도 친절한 상담원 말투를 잘 학습함.
3.  **RAG 적합성:** 질문 의도를 파악하고 약관 스타일로 답변하는 능력이 입증되었으므로, RAG 파이프라인의 생성기(Generator)로 적합함.

### 5.2 다음 단계 (Next Steps)

1.  **RAG + Fine-Tuned LLM 통합 (우선순위 1):**
    *   모델 단독 추론의 한계(약관 조항 번호 정확도 부족)를 해결하기 위해, 검색된 약관 내용(Context)을 프롬프트에 포함하여 최종 답변을 생성하는 End-to-End 파이프라인 구축.

2.  **Ragas 정량 평가 (우선순위 2):**
    *   통합 시스템에 대해 Faithfulness(환각 여부), Answer Relevance(답변 관련성) 점수를 수치화하여 그래프로 제시.

3.  **Streamlit 데모 구축 (우선순위 3):**
    *   실제 웹 화면에서 채팅하듯 보험 상담을 할 수 있는 데모 페이지 제작.

---

---

## 6. RAG 통합 및 3개 모델 비교 평가 (RAG Integration & 3-Model Comparison)

**작성 일시:** 2025-11-24 13:20 (KST)

### 6.1 실험 설계 (Experiment Design)

#### 6.1.1 후보 선정 사유 (Why Candidates)

1.  **Base Model (단독):** `beomi/Llama-3-Open-Ko-8B`
    *   **선정 이유:** 파인튜닝 전 성능을 확인하기 위한 기준점.

2.  **Fine-Tuned Model v3 (단독):** `llama-3-ko-insurance-lora-v3`
    *   **선정 이유:** 파인튜닝만으로 달성 가능한 성능의 상한선을 확인.

3.  **RAG + Fine-Tuned Model v3 (Context 포함):**
    *   **Retriever:** Mecab BM25 (Top-3 문서 검색)
    *   **Generator:** Fine-Tuned Model v3
    *   **선정 이유:** RAG 통합이 모델 단독 추론의 한계를 얼마나 보완하는지 검증.

#### 6.1.2 실험 규모 (Experiment Scale)
*   **데이터셋:** `test_20.json` (30개 샘플 무작위 추출)
*   **평가 스크립트:** `06_Evaluation/compare_three_models.py`
*   **결과 파일:** `06_Evaluation/comparison_3_models.json`

### 6.2 검증 결과 (Validation)

#### 6.2.1 정량 평가 (Quantitative Evaluation)

*   **평가 샘플 수:** 30개
*   **실행 시간:** 약 26분
*   **결과 파일 크기:** 107KB

> **참고:** 정량 지표(Accuracy, BLEU, ROUGE 등)는 추후 Ragas 라이브러리로 추가 측정 예정.

#### 6.2.2 정성 평가 (Qualitative Evaluation) - 30개 샘플 분석

##### **6.2.2.1 RAG 통합이 성능을 향상시킨 사례 (RAG Success Cases)**

| 유형 | 질문 요약 | Base | Fine-Tuned | RAG+Fine-Tuned | 평가 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Fact** | 걸음수 기준 구체적 수치 | "가입 시점에 충족해야 하는 기준입니다." (모호) | "구체적인 수치가 명시되어 있지 않습니다." (오답) | **"1개월 동안 하루 평균 5천보 이상"** (정확) | **RAG 승** (검색된 약관에서 정확한 수치 발견) |
| **Fact** | 가지급금 지급 거절 통지 기간 | 질문 반복 (오답) | "구체적인 기간이 명시되어 있지 않습니다." (오답) | **"10일 이내"** (정확) | **RAG 승** (약관 조항에서 정확한 기간 발견) |
| **Fact** | 자동차보험 처음 가입 기간 | "30일입니다." (오답) | "명시되어 있지 않습니다." (오답) | **"10일 이내"** (정확) | **RAG 승** (검색된 약관에서 정확한 기간 발견) |
| **Fact** | 출·퇴근 시간대 | "오전 7시~9시, 오후 6시~8시" (정확) | "명시되어 있지 않습니다." (오답) | **"오전 7시~9시, 오후 6시~8시"** (정확) | **RAG 승** (Base는 우연히 맞췄으나, RAG는 약관 기반으로 정확) |
| **Fact** | 소득세법 보험료 공제 한도 | "장애인 300만원, 그 외 100만원" (오답) | "명시되어 있지 않습니다." (오답) | **"각각 연 100만원"** (정확) | **RAG 승** (검색된 약관에서 정확한 금액 발견) |

##### **6.2.2.2 RAG 통합이 실패한 사례 (RAG Failure Cases)**

| 유형 | 질문 요약 | Base | Fine-Tuned | RAG+Fine-Tuned | 문제점 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Scenario** | 노트북 액정 파손 (200만원 한도) | "보상 대상이 아닙니다." (오답) | "보상이 어렵습니다." (오답) | **검색된 Context가 관련 없음** (오답) | **검색 실패** (BM25가 관련 약관을 찾지 못함) |
| **Scenario** | 아내 차에 치이는 사고 | 반복 생성 (오답) | "보상 가능할 수 있습니다." (오답) | **검색된 Context가 관련 없음** (오답) | **검색 실패** (부정 사례를 찾지 못함) |
| **Scenario** | 전업주부 휴업손해 | 반복 생성 (오답) | "빠른 쾌유를 바랍니다." (불완전) | **검색된 Context가 관련 없음** (오답) | **검색 실패** (가사종사자 관련 약관을 찾지 못함) |

##### **6.2.2.3 공통 실패 사례 (Common Failure Cases)**

| 유형 | 질문 요약 | Base | Fine-Tuned | RAG+Fine-Tuned | 문제점 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Scenario** | 자기신체사고 2배 보장 (후유장애 7급) | "6천만원입니다." (오답) | "6천만원입니다." (오답) | "공제액이 없는 경우..." (오답) | **공통 실패** (정답: 2,400만원. 검색된 약관이 부정확하거나 모델이 잘못 해석) |
| **Fact** | 5급 상해 한도금액 | "1억원입니다." (오답) | "명시되어 있지 않습니다." (오답) | "명시되어 있지 않습니다." (오답) | **공통 실패** (정답: 900만원. 검색된 약관에 해당 정보 없음) |

### 6.3 실패 분석 (Failure Analysis)

#### 6.3.1 RAG 통합의 한계

1.  **검색 정확도 문제:**
    *   BM25 리트리버가 관련 약관을 찾지 못하는 경우가 있음.
    *   **원인:** 질문과 약관 문서 간 키워드 매칭 실패 (예: "노트북 파손" vs "휴대품 손해").
    *   **해결책:** Hybrid Retrieval (Dense + Sparse) 또는 Reranker 도입 검토.

2.  **Context 품질 문제:**
    *   검색된 문서가 질문과 관련이 없거나, 필요한 정보가 포함되지 않은 경우.
    *   **원인:** Top-3 문서만 사용하여 관련 문서를 놓칠 수 있음.
    *   **해결책:** Top-K를 늘리거나, Reranker로 관련성 높은 문서만 선별.

3.  **모델의 Context 활용 능력:**
    *   검색된 약관이 정확해도 모델이 잘못 해석하는 경우 (예: 자기신체사고 2배 보장).
    *   **원인:** 복잡한 계산이나 조건문을 처리하는 능력 부족.
    *   **해결책:** 프롬프트 엔지니어링 개선 또는 Chain-of-Thought (CoT) 기법 적용.

#### 6.3.2 RAG 통합의 효과

1.  **구체적 수치/기간 정확도 향상:**
    *   "10일 이내", "5천보 이상" 같은 정확한 수치를 약관에서 찾아 정확히 답변.
    *   **효과:** Fact 유형 질문에서 정확도가 크게 향상됨.

2.  **약관 조항 번호 인용:**
    *   검색된 약관에 조항 번호가 포함되어 있으면 정확히 인용 가능.
    *   **효과:** 전문성과 신뢰도 향상.

### 6.4 최종 결론 (Final Conclusion)

#### **채택 모델:** **RAG + Fine-Tuned Model v3**

#### **채택 근거:**
1.  **정확도 향상:** Fact 유형 질문에서 모델 단독 대비 정확도가 크게 향상됨 (예: "10일 이내", "5천보 이상" 등).
2.  **약관 기반 답변:** 검색된 약관을 근거로 답변하므로 환각(Hallucination) 위험이 낮음.
3.  **확장성:** 새로운 약관이 추가되어도 벡터 DB만 업데이트하면 되므로 재학습 불필요.

#### **개선 필요 사항:**
1.  **검색 정확도 향상:** Hybrid Retrieval 또는 Reranker 도입.
2.  **부정 사례 처리:** 학습 데이터에 부정 사례 비율 증가 (v4 파인튜닝 검토).
3.  **Context 활용 능력:** 프롬프트 엔지니어링 개선.

---

## 7. 참고 자료 (References)

*   **2개 모델 비교 결과:** `05_FineTuning/comparison_base_vs_finetuned.json`
*   **3개 모델 비교 결과:** `06_Evaluation/comparison_3_models.json`
*   **평가 스크립트:** 
    *   `05_FineTuning/compare_base_vs_finetuned.py`
    *   `06_Evaluation/compare_three_models.py`
*   **RAG 통합 스크립트:** `04_LLM/rag_finetuned_v3.py`
*   **테스트 데이터셋:** `05_FineTuning/test_20.json`
*   **관련 문서:** `05_FineTuning/README_FineTuning_Log.md`

