import json
import os
import glob
from pathlib import Path

def validate_and_merge(input_dir, output_file):
    print(f"ğŸ” ë°ì´í„° í†µí•© ë° ê²€ì¦ ì‹œì‘: {input_dir}")
    
    all_files = sorted(glob.glob(os.path.join(input_dir, "dataset_part_*.json")))
    print(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
    
    merged_data = []
    seen_instructions = set()
    stats = {
        "total_read": 0,
        "valid": 0,
        "filtered_short": 0,    # ë„ˆë¬´ ì§§ìŒ
        "filtered_incomplete": 0, # ë¬¸ì¥ ì•ˆ ëë‚¨
        "filtered_duplicate": 0,  # ì¤‘ë³µ ì§ˆë¬¸
        "filtered_no_answer": 0,  # "ì—†ìŠµë‹ˆë‹¤" ë¥˜
        "types": {"fact": 0, "scenario": 0, "easy": 0, "unknown": 0}
    }
    
    # "ì—†ìŠµë‹ˆë‹¤" ë¥˜ì˜ ë¬´ì˜ë¯¸í•œ ë‹µë³€ íŒ¨í„´
    invalid_patterns = [
        "ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤",
        "í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
        "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        "ì œê³µëœ ì•½ê´€ì—ëŠ”",
        "ì–¸ê¸‰ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
    ]

    for file_path in all_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                stats["total_read"] += len(data)
                
                for item in data:
                    instruction = item.get('instruction', '').strip()
                    output = item.get('output', '').strip()
                    q_type = item.get('type', 'unknown')
                    
                    # 1. í•„ìˆ˜ í•„ë“œ ì²´í¬
                    if not instruction or not output:
                        continue
                        
                    # 2. ì¤‘ë³µ ì²´í¬
                    if instruction in seen_instructions:
                        stats["filtered_duplicate"] += 1
                        continue
                    
                    # 3. ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ì€ ë‹µë³€ ì œì™¸)
                    if len(output) < 10:
                        stats["filtered_short"] += 1
                        continue
                        
                    # 4. ì™„ê²°ì„± ì²´í¬ (ë¬¸ì¥ì´ ì˜ë ¸ëŠ”ì§€ í™•ì¸)
                    # í•œê¸€/ì˜ë¬¸ ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
                    if not output[-1] in ['.', '!', '?', '"', "'", 'ë‹¤', 'ìš”', 'ì£ ']:
                        stats["filtered_incomplete"] += 1
                        continue
                        
                    # 5. ìœ íš¨ì„± ì²´í¬ (ë¬´ì˜ë¯¸í•œ ë‹µë³€ ì œì™¸)
                    # outputì´ invalid_patterns ì¤‘ í•˜ë‚˜ë¥¼ í¬í•¨í•˜ê³ , ê¸¸ì´ê°€ ì§§ìœ¼ë©´(ì˜ˆ: 50ì ë¯¸ë§Œ) ì œì™¸
                    is_invalid = False
                    if len(output) < 60:
                        for pattern in invalid_patterns:
                            if pattern in output:
                                is_invalid = True
                                break
                    
                    if is_invalid:
                        stats["filtered_no_answer"] += 1
                        continue

                    # í†µê³¼
                    seen_instructions.add(instruction)
                    merged_data.append(item)
                    stats["valid"] += 1
                    stats["types"][q_type] = stats["types"].get(q_type, 0) + 1
                    
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")

    # ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
    print("\n" + "="*50)
    print("ğŸ“Š [ê²€ì¦ ë° í†µí•© ê²°ê³¼]")
    print(f"ì´ ì½ì€ ë°ì´í„°: {stats['total_read']:,}ê°œ")
    print(f"âœ… ìµœì¢… ìœ íš¨ ë°ì´í„°: {stats['valid']:,}ê°œ")
    print("-" * 30)
    print(f"ğŸ—‘ï¸ [í•„í„°ë§ ë‚´ì—­]")
    print(f"  - ì¤‘ë³µ ì§ˆë¬¸ ì œê±°: {stats['filtered_duplicate']:,}ê°œ")
    print(f"  - ë„ˆë¬´ ì§§ì€ ë‹µë³€: {stats['filtered_short']:,}ê°œ")
    print(f"  - ë¬¸ì¥ ë¶ˆì™„ì „(ì˜ë¦¼): {stats['filtered_incomplete']:,}ê°œ")
    print(f"  - ë¬´ì˜ë¯¸í•œ ë‹µë³€(ì—†ìŒ ë“±): {stats['filtered_no_answer']:,}ê°œ")
    print("-" * 30)
    print(f"ğŸ“ˆ [ìœ í˜•ë³„ ë¶„í¬]")
    print(f"  - Fact: {stats['types']['fact']:,}")
    print(f"  - Scenario: {stats['types']['scenario']:,}")
    print(f"  - Easy: {stats['types']['easy']:,}")
    print("="*50)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file}")

if __name__ == "__main__":
    # ê²½ë¡œ ì„¤ì •
    BASE_DIR = Path("/home/pencilfoxs/0_Insurance_PF/05_FineTuning")
    INPUT_DIR = BASE_DIR / "generated_data_v2"
    OUTPUT_FILE = BASE_DIR / "train_dataset_final_v2.json"
    
    validate_and_merge(INPUT_DIR, OUTPUT_FILE)
