import json
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import PeftModel
from tqdm import tqdm

# ì„¤ì •
BASE_MODEL_ID = "beomi/Llama-3-Open-Ko-8B"
ADAPTER_MODEL_DIR = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/llama-3-ko-insurance-lora-v3"
TEST_DATA_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/test_20.json"
OUTPUT_FILE = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/comparison_base_vs_finetuned.json"

# ìƒ˜í”Œë§: ì „ì²´ 1,088ê°œ ì¤‘ 50ê°œë§Œ ë¹„êµ (ë¹ ë¥¸ í™•ì¸ìš©)
SAMPLE_SIZE = 50
MAX_NEW_TOKENS = 512

def format_prompt(instruction, input_text=""):
    """í•™ìŠµ í¬ë§·ê³¼ ë™ì¼í•˜ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    if input_text:
        return f"### ì§€ì‹œ\n{instruction}\n\n### ì…ë ¥\n{input_text}\n\n### ì¶œë ¥\n"
    else:
        return f"### ì§€ì‹œ\n{instruction}\n\n### ì…ë ¥\n{input_text}\n\n### ì¶œë ¥\n"

def generate_answer(model, tokenizer, prompt):
    """ëª¨ë¸ ë‹µë³€ ìƒì„±"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=0.1,
            top_p=0.9,
            repetition_penalty=1.2,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    input_len = inputs.input_ids.shape[1]
    generated_tokens = outputs[:, input_len:]
    answer = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()
    return answer

def main():
    print("ğŸš€ [Start] Base Model vs Fine-Tuned Model Comparison")
    
    # ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§
    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # ëœë¤ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •)
    import random
    random.seed(42)
    sampled_data = random.sample(test_data, min(SAMPLE_SIZE, len(test_data)))
    
    print(f"ğŸ“‚ Loaded {len(test_data)} samples, sampling {len(sampled_data)} for comparison.")
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # 1. Base Model ë¡œë“œ
    print("\nğŸ¤– [1/2] Loading Base Model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    base_model.eval()
    base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
    base_tokenizer.pad_token = base_tokenizer.eos_token
    base_tokenizer.padding_side = "left"
    print("âœ… Base Model Loaded!")
    
    # 2. Fine-Tuned Model ë¡œë“œ
    print("\nğŸ¤– [2/2] Loading Fine-Tuned Model...")
    finetuned_model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_DIR)
    finetuned_model.eval()
    finetuned_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token
    finetuned_tokenizer.padding_side = "left"
    print("âœ… Fine-Tuned Model Loaded!")
    
    # 3. ë¹„êµ ì¶”ë¡ 
    print(f"\nğŸ”„ Generating answers for {len(sampled_data)} samples...")
    results = []
    
    for item in tqdm(sampled_data, desc="Comparing"):
        prompt = format_prompt(item['instruction'], item['input'])
        
        # Base Model ë‹µë³€
        base_answer = generate_answer(base_model, base_tokenizer, prompt)
        
        # Fine-Tuned Model ë‹µë³€
        finetuned_answer = generate_answer(finetuned_model, finetuned_tokenizer, prompt)
        
        result = {
            "type": item.get("type", "unknown"),
            "instruction": item["instruction"],
            "input": item["input"],
            "ground_truth": item["output"],
            "base_model_answer": base_answer,
            "finetuned_model_answer": finetuned_answer,
        }
        results.append(result)
    
    # 4. ê²°ê³¼ ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ Comparison results saved to {OUTPUT_FILE}")
    
    # 5. ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ” [Sample Comparison]")
    print("=" * 80)
    sample = results[0]
    print(f"\nğŸ“‹ Q: {sample['instruction'][:100]}...")
    print(f"\nâœ… ì •ë‹µ: {sample['ground_truth'][:150]}...")
    print(f"\nğŸ”µ Base Model: {sample['base_model_answer'][:150]}...")
    print(f"\nğŸŸ¢ Fine-Tuned: {sample['finetuned_model_answer'][:150]}...")
    
    print("\nâœ… Comparison Completed!")

if __name__ == "__main__":
    main()

