"""
QLoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (RAG ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ìš© - ê°œì„  ë²„ì „)
4-bit ì–‘ìí™”ëœ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ í•™ìŠµì‹œì¼œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹
ê°œì„  ì‚¬í•­: EOS í† í° ì²˜ë¦¬ ê°•í™”, pad_tokenê³¼ eos_token ë¶„ë¦¬
"""

import os
import torch
import json
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# 1. ì„¤ì • (Configuration)
BASE_MODEL = "beomi/Llama-3-Open-Ko-8B"
DATASET_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/train_dataset_final_v2.json"
OUTPUT_DIR = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/llama-3-ko-insurance-lora-v2"

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (SPEC ë¬¸ì„œ ë°˜ì˜)
EPOCHS = 3
BATCH_SIZE = 4
GRAD_ACCUMULATION = 4
LEARNING_RATE = 2e-4
MAX_SEQ_LENGTH = 2048
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05

def main():
    print("ğŸš€ [Start] QLoRA Fine-Tuning Phase 3 (Scale-Up)")
    
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§·íŒ…
    print(f"ğŸ“‚ Loading Dataset: {DATASET_PATH}")
    with open(DATASET_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ… (Instruction Tuning Format)
    formatted_data = []
    for item in data:
        text = (
            f"### ì§€ì‹œ\n{item['instruction']}\n\n"
            f"### ì…ë ¥\n{item['input']}\n\n"
            f"### ì¶œë ¥\n{item['output']}<|end_of_text|>"
        )
        formatted_data.append({"text": text})
    
    # HuggingFace Dataset ë³€í™˜
    dataset = Dataset.from_list(formatted_data)
    print(f"âœ… Dataset Size: {len(dataset)} samples")
    
    # í¬ë§·íŒ… í•¨ìˆ˜ (ì´ë¯¸ í¬ë§·íŒ…ëœ ë°ì´í„°ì´ë¯€ë¡œ ë‹¨ìˆœ ë°˜í™˜)
    def formatting_prompts_func(example):
        return example["text"]

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ì„¤ì •)
    print("ğŸ¤– Loading Model & Tokenizer...")
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # k-bit í•™ìŠµ ì¤€ë¹„
    model = prepare_model_for_kbit_training(model)

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token  # Pad token ì„¤ì •
    tokenizer.padding_side = "right"  # FP16 í•™ìŠµ ì‹œ right padding ê¶Œì¥

    # 4. LoRA ì„¤ì •
    peft_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] # ëª¨ë“  Linear ë ˆì´ì–´
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 5. í•™ìŠµ ì¸ì (Training Arguments)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUMULATION,
        learning_rate=LEARNING_RATE,
        weight_decay=0.001,
        fp16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=500,  # 500 stepë§ˆë‹¤ ì €ì¥
        warmup_ratio=0.03,
        group_by_length=True,
        lr_scheduler_type="cosine",
        report_to="tensorboard",
    )

    # 6. Trainer ì„¤ì • (SFTTrainer)
    # max_seq_lengthëŠ” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ SFTTrainerì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        formatting_func=formatting_prompts_func,
    )

    # 7. í•™ìŠµ ì‹œì‘
    print("ğŸ”¥ Training Start!")
    trainer.train()

    # 8. ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ Saving Model to {OUTPUT_DIR}")
    trainer.model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("âœ… Fine-Tuning Completed Successfully!")

if __name__ == "__main__":
    main()

