import os
import torch
import json
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# 1. ì„¤ì • (Configuration)
BASE_MODEL = "beomi/Llama-3-Open-Ko-8B"
# [v4] Stratified Splitëœ í•™ìŠµ ë°ì´í„° ì‚¬ìš©
DATASET_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/train_v4.json"
# [v4] ì¶œë ¥ ê²½ë¡œ
OUTPUT_DIR = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/llama-3-ko-insurance-lora-v4"

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (v4 ë°ì´í„° ê·œëª¨ ê³ ë ¤)
EPOCHS = 3
BATCH_SIZE = 4
GRAD_ACCUMULATION = 4
LEARNING_RATE = 2e-4
MAX_SEQ_LENGTH = 2048
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05

def main():
    print("ğŸš€ [Start] QLoRA Fine-Tuning Phase 5 (v4 Dataset)")
    print(f"ğŸ“Œ Base Model: {BASE_MODEL}")
    print(f"ğŸ“Œ Dataset: {DATASET_PATH}")
    print(f"ğŸ“Œ Output Dir: {OUTPUT_DIR}")
    
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§·íŒ…
    print(f"ğŸ“‚ Loading Dataset...")
    if not os.path.exists(DATASET_PATH):
        print(f"âŒ Error: Dataset file not found at {DATASET_PATH}")
        return

    with open(DATASET_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ… (Instruction Tuning Format)
    formatted_data = []
    for item in data:
        # v4 ë°ì´í„°ì˜ í•„ë“œ ì‚¬ìš© (instruction, input, output)
        text = (
            f"### ì§€ì‹œ\n{item['instruction']}\n\n"
            f"### ì…ë ¥\n{item['input']}\n\n"
            f"### ì¶œë ¥\n{item['output']}<|end_of_text|>"
        )
        formatted_data.append({"text": text})

    # HuggingFace Dataset ë³€í™˜
    dataset = Dataset.from_list(formatted_data)
    print(f"âœ… Dataset Size: {len(dataset)} samples")

    # í¬ë§·íŒ… í•¨ìˆ˜
    def formatting_prompts_func(example):
        return example["text"]

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ì„¤ì •)
    print("ğŸ¤– Loading Model & Tokenizer...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    model = prepare_model_for_kbit_training(model)

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # 4. LoRA ì„¤ì •
    peft_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 5. í•™ìŠµ ì¸ì
    # ë°ì´í„°ê°€ ëŠ˜ì–´ë‚¬ìœ¼ë¯€ë¡œ save_stepsë¥¼ ì¡°ê¸ˆ ëŠ˜ë ¤ì¤ë‹ˆë‹¤ (200 -> 400)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUMULATION,
        learning_rate=LEARNING_RATE,
        weight_decay=0.001,
        fp16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=400,
        warmup_ratio=0.03,
        group_by_length=True,
        lr_scheduler_type="cosine",
        report_to="tensorboard",
    )

    # 6. Trainer ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        formatting_func=formatting_prompts_func,
    )

    # 7. í•™ìŠµ ì‹œì‘
    print("ğŸ”¥ Training Start!")
    trainer.train()

    # 8. ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ Saving Model to {OUTPUT_DIR}")
    trainer.model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("âœ… Fine-Tuning Completed Successfully!")

if __name__ == "__main__":
    main()
