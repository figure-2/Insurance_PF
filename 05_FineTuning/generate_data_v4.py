"""
v4 íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ìƒì„± (3,800ê°œ - í™•ì¥ ë²„ì „)
- ë¶€ì • ì‚¬ë¡€: 760ê°œ (20%)
- ê¸ì • ì‚¬ë¡€: 1,900ê°œ (50%)
- ë³µì¡í•œ ê³„ì‚° ì‹œë‚˜ë¦¬ì˜¤: 570ê°œ (15%)
- ì¡°í•­ ë²ˆí˜¸ ëª…ì‹œ: 380ê°œ (10%)
- ë™ì˜ì–´/ê²€ìƒ‰ ì‹¤íŒ¨ ëŒ€ì‘: 190ê°œ (5%)
- API: Google Gemini 2.0 Flash (ì†ë„ ìµœì í™”)
"""

import json
import random
import argparse
import os
import time
import requests
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
from collections import defaultdict

# ì„¤ì •
DATA_PATH = "/home/pencilfoxs/0_Insurance_PF/01_Preprocessing/chunked_data.jsonl"
OUTPUT_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/train_dataset_v4_negative_enhanced.json"
GEMINI_MODEL = "gemini-2.0-flash"  # ë³€ê²½: exp -> flash (ì†ë„ í–¥ìƒ)
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"

# ëª©í‘œ ìƒ˜í”Œ ìˆ˜
TARGET_NEGATIVE = 760
TARGET_POSITIVE = 1900
TARGET_CALCULATION = 570
TARGET_ARTICLE = 380
TARGET_SYNONYM = 190
TARGET_TOTAL = 3800


def load_api_key():
    """API í‚¤ ë¡œë“œ"""
    env_paths = ['/home/pencilfoxs/00_new/.env2', '/home/pencilfoxs/PJ/.env2']
    
    for path in env_paths:
        if os.path.exists(path):
            load_dotenv(path)
    
    api_key = os.getenv("GOOGLE_API_KEY")
    
    if not api_key:
        for path in env_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    for line in f:
                        clean_line = line.split('#')[0].strip()
                        if clean_line.startswith("GOOGLE_API_KEY="):
                            api_key = clean_line.split('=', 1)[1].strip()
                            break
            if api_key:
                break
    
    if not api_key:
        raise ValueError("GOOGLE_API_KEY not found in .env2 files.")
    
    return api_key


def classify_chunk(chunk_text: str) -> str:
    """ì²­í¬ë¥¼ ë¶€ì •/ê¸ì •/ê³„ì‚°/ì¡°í•­ìœ¼ë¡œ ë¶„ë¥˜"""
    # ë¶€ì • í‚¤ì›Œë“œ
    negative_keywords = [
        "ë³´ìƒí•˜ì§€ ì•Š", "ì œì™¸", "ë©´ì±…", "ë¶ˆê°€", "ë¶ˆê°€ëŠ¥", 
        "ê±°ì ˆ", "í•´ì§€", "ë¬´íš¨", "ë³´ìƒí•˜ì§€ ì•„ë‹ˆ", "ë³´ìƒí•˜ì§€ ì•ŠëŠ”"
    ]
    
    # ê¸ì • í‚¤ì›Œë“œ
    positive_keywords = [
        "ë³´ìƒ", "ì§€ê¸‰", "ê°€ëŠ¥", "ì ìš©", "ì§€ì›", "ë³´ì¥"
    ]
    
    # ê³„ì‚° ê´€ë ¨ í‚¤ì›Œë“œ
    calculation_keywords = [
        "ë°°ìœ¨", "ê³µì œì•¡", "í•œë„", "ì§€ê¸‰ë³´í—˜ê¸ˆ", "ê³„ì‚°", "ì‚°ì¶œ",
        "ë°°ë¶„", "ë¹„ìœ¨", "ë°°ìˆ˜", "ê³±í•˜ê¸°", "ë‚˜ëˆ„ê¸°", "í•©ê³„"
    ]
    
    # ì¡°í•­ ë²ˆí˜¸ í‚¤ì›Œë“œ
    article_keywords = [
        "ì œ", "ì¡°", "í•­", "í˜¸", "ëª©", "ë³„í‘œ", "ì²¨ë¶€"
    ]
    
    # ìš°ì„ ìˆœìœ„: ë¶€ì • > ê³„ì‚° > ì¡°í•­ > ê¸ì •
    if any(kw in chunk_text for kw in negative_keywords) and (
        "ë³´ìƒí•˜ì§€ ì•Š" in chunk_text or "ì œì™¸" in chunk_text or "ë©´ì±…" in chunk_text
    ):
        return "negative"
    
    if any(kw in chunk_text for kw in calculation_keywords):
        return "calculation"
    
    if any(kw in chunk_text for kw in article_keywords) and (
        "ì œ" in chunk_text and ("ì¡°" in chunk_text or "í•­" in chunk_text)
    ):
        return "article"
    
    return "positive"


def call_gemini_api(api_key: str, payload: dict, max_retries: int = 5):
    """Gemini API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    for attempt in range(max_retries):
        try:
            response = requests.post(
                f"{GEMINI_API_URL}?key={api_key}",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    content = result['candidates'][0]['content']['parts'][0]['text']
                    return content
            elif response.status_code == 429:  # Rate limit
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"Rate limit hit. Waiting {wait_time:.2f}s...")
                time.sleep(wait_time)
                continue
            else:
                print(f"API Error {response.status_code}: {response.text}")
                time.sleep(2 ** attempt)
                continue
                
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            continue
    
    return None


def generate_negative_qa(chunk_text: str, api_key: str) -> dict | None:
    """ë¶€ì • ì‚¬ë¡€ QA ìƒì„±"""
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€]ì—ëŠ” "ë³´ìƒí•˜ì§€ ì•ŠëŠ”ë‹¤", "ì œì™¸í•œë‹¤", "ë©´ì±…" ë“± **ë¶€ì •ì ì¸ ë‚´ìš©**ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì¤‘ìš”:** ì•½ê´€ì— "ë³´ìƒí•˜ì§€ ì•ŠëŠ”ë‹¤", "ì œì™¸í•œë‹¤" ê°™ì€ ë¶€ì • í‘œí˜„ì´ ìˆìœ¼ë©´, ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ë©° **"ë³´ìƒì´ ì–´ë µìŠµë‹ˆë‹¤"**, **"ë³´ìƒë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"** ê°™ì€ ë¶€ì • í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ë³´í—˜ ì•½ê´€]
{chunk_text[:2000]}

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ (ì˜ˆ: "ì´ ê²½ìš° ë³´ìƒë°›ì„ ìˆ˜ ìˆë‚˜ìš”?")
2. ë‹µë³€: ì•½ê´€ì— "ë³´ìƒí•˜ì§€ ì•ŠëŠ”ë‹¤", "ì œì™¸í•œë‹¤" ë“±ì´ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´, ë°˜ë“œì‹œ **"ì•ˆíƒ€ê¹ê²Œë„ ë³´ìƒì´ ì–´ë µìŠµë‹ˆë‹¤"**, **"ë³´ìƒë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"** ê°™ì€ ë¶€ì • í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš©]
ë‹µë³€: [ì•½ê´€ì„ ì°¸ì¡°í•œ ë‹µë³€ë‚´ìš© - ë¶€ì • í‘œí˜„ ëª…í™•íˆ í¬í•¨]

### ìƒì„± ê²°ê³¼
"""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 800
        }
    }
    
    output = call_gemini_api(api_key, payload)
    if not output:
        return None
    
    # íŒŒì‹±
    lines = output.strip().split('\n')
    question = None
    answer = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
            question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
        elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
            answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
            # ë‹µë³€ì€ ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆìŒ
            answer_lines = [answer]
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if next_line and not next_line.startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                    answer_lines.append(next_line)
                else:
                    break
            answer = '\n'.join(answer_lines)
            break
    
    if question and answer:
        # ë¶€ì • í‘œí˜„ ê²€ì¦
        negative_phrases = ["ë³´ìƒì´ ì–´ë µ", "ë³´ìƒë°›ì„ ìˆ˜ ì—†", "ë³´ìƒí•˜ì§€ ì•Š", "ì œì™¸", "ë©´ì±…"]
        if any(phrase in answer for phrase in negative_phrases):
            return {
                "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
                "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
                "output": answer,
                "type": "negative"
            }
    
    return None


def generate_positive_qa(chunk_text: str, api_key: str) -> dict | None:
    """ê¸ì • ì‚¬ë¡€ QA ìƒì„±"""
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì¼ë°˜ ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ (ì˜ˆ: "ì´ê±° ë³´ìƒ ë˜ë‚˜ìš”?", "ì¹¨ìˆ˜ëëŠ”ë° ì–´ë–¡í•˜ì£ ?")
2. ë‹µë³€: ë°˜ë“œì‹œ ì œê³µëœ ì•½ê´€ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€. ì•½ê´€ì— ëª…ì‹œëœ ì¡°í•­ì´ë‚˜ ê·¼ê±°ë¥¼ ì–¸ê¸‰í•˜ë©° ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª….
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš©]
ë‹µë³€: [ì•½ê´€ì„ ì°¸ì¡°í•œ ë‹µë³€ë‚´ìš©]

[ë³´í—˜ ì•½ê´€]
{chunk_text[:2000]}

### ìƒì„± ê²°ê³¼
"""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 800
        }
    }
    
    output = call_gemini_api(api_key, payload)
    if not output:
        return None
    
    # íŒŒì‹±
    lines = output.strip().split('\n')
    question = None
    answer = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
            question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
        elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
            answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
            answer_lines = [answer]
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if next_line and not next_line.startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                    answer_lines.append(next_line)
                else:
                    break
            answer = '\n'.join(answer_lines)
            break
    
    if question and answer:
        return {
            "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
            "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
            "output": answer,
            "type": "positive"
        }
    
    return None


def generate_calculation_qa(chunk_text: str, api_key: str) -> dict | None:
    """ë³µì¡í•œ ê³„ì‚° ì‹œë‚˜ë¦¬ì˜¤ QA ìƒì„±"""
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€]ì—ëŠ” ë³´í—˜ê¸ˆ ê³„ì‚° ê³µì‹, ë°°ìœ¨, ê³µì œì•¡, í•œë„ ë“± **ê³„ì‚°ì´ í•„ìš”í•œ ë‚´ìš©**ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì¤‘ìš”:** ì•½ê´€ì— ëª…ì‹œëœ ê³„ì‚° ê³µì‹(ì˜ˆ: ì§€ê¸‰ë³´í—˜ê¸ˆ = ì‹¤ì œì†í•´ì•¡ + ë¹„ìš© - ê³µì œì•¡)ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì˜ ë³´í—˜ê¸ˆì„ ê³„ì‚°í•˜ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ë³´í—˜ ì•½ê´€]
{chunk_text[:2000]}

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ê°€ í¬í•¨ëœ ì‹œë‚˜ë¦¬ì˜¤ ì§ˆë¬¸ (ì˜ˆ: "ë³´í—˜ê°€ì…ê¸ˆì•¡ 3ì²œë§Œì›, í›„ìœ ì¥ì•  7ê¸‰ì¼ ë•Œ ë³´í—˜ê¸ˆì€?")
2. ë‹µë³€: ì•½ê´€ì˜ ê³„ì‚° ê³µì‹ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , ìµœì¢… ê¸ˆì•¡ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”.
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš©]
ë‹µë³€: [ë‹¨ê³„ë³„ ê³„ì‚° ê³¼ì • + ìµœì¢… ê¸ˆì•¡]

### ìƒì„± ê²°ê³¼
"""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 1000
        }
    }
    
    output = call_gemini_api(api_key, payload)
    if not output:
        return None
    
    # íŒŒì‹±
    lines = output.strip().split('\n')
    question = None
    answer = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
            question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
        elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
            answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
            answer_lines = [answer]
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if next_line and not next_line.startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                    answer_lines.append(next_line)
                else:
                    break
            answer = '\n'.join(answer_lines)
            break
    
    if question and answer:
        # ê³„ì‚° ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì¦
        calculation_keywords = ["ë§Œì›", "ì›", "ê³„ì‚°", "ë°°ìœ¨", "ê³µì œ", "í•œë„", "í•©ê³„", "ê³±í•˜ê¸°", "ë‚˜ëˆ„ê¸°"]
        if any(kw in answer for kw in calculation_keywords):
            return {
                "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
                "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
                "output": answer,
                "type": "calculation"
            }
    
    return None


def generate_article_qa(chunk_text: str, api_key: str) -> dict | None:
    """ì¡°í•­ ë²ˆí˜¸ ëª…ì‹œ QA ìƒì„±"""
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€]ì—ëŠ” "ì œXì¡°", "ì œXí•­" ê°™ì€ **ì¡°í•­ ë²ˆí˜¸**ê°€ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì¤‘ìš”:** ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì•½ê´€ì˜ ì¡°í•­ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "ì•½ê´€ ì œ16ì¡° 4í•­ì— ë”°ë¥´ë©´...").

[ë³´í—˜ ì•½ê´€]
{chunk_text[:2000]}

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: ì¡°í•­ ë²ˆí˜¸ë¥¼ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸ ë˜ëŠ” ì¡°í•­ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸
2. ë‹µë³€: ë°˜ë“œì‹œ "ì•½ê´€ ì œXì¡°", "ì œXí•­" ê°™ì€ ì¡°í•­ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš©]
ë‹µë³€: [ì¡°í•­ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•œ ë‹µë³€ë‚´ìš©]

### ìƒì„± ê²°ê³¼
"""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 800
        }
    }
    
    output = call_gemini_api(api_key, payload)
    if not output:
        return None
    
    # íŒŒì‹±
    lines = output.strip().split('\n')
    question = None
    answer = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
            question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
        elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
            answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
            answer_lines = [answer]
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if next_line and not next_line.startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                    answer_lines.append(next_line)
                else:
                    break
            answer = '\n'.join(answer_lines)
            break
    
    if question and answer:
        # ì¡°í•­ ë²ˆí˜¸ ê²€ì¦
        if "ì œ" in answer and ("ì¡°" in answer or "í•­" in answer):
            return {
                "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
                "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
                "output": answer,
                "type": "article"
            }
    
    return None


def generate_synonym_qa(chunk_text: str, api_key: str, synonym_dict: dict = None) -> dict | None:
    """ë™ì˜ì–´/ê²€ìƒ‰ ì‹¤íŒ¨ ëŒ€ì‘ QA ìƒì„±"""
    # ë™ì˜ì–´ ì‚¬ì „ (í™•ì¥ ë²„ì „ - ë³´í—˜ ì•½ê´€ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì¤‘ì‹¬)
    if synonym_dict is None:
        synonym_dict = {
            # ê¸°ì¡´ í‚¤ì›Œë“œ
            "ë…¸íŠ¸ë¶": ["íœ´ëŒ€ìš© ì»´í“¨í„°", "ë©í†±", "ë…¸íŠ¸ë¶ PC", "ë…¸íŠ¸ë¶ ì»´í“¨í„°"],
            "ì•„ë‚´": ["ë°°ìš°ì", "ë¶€ì¸", "ì²˜", "ì™€ì´í”„"],
            "ì „ì—…ì£¼ë¶€": ["ê°€ì‚¬ì¢…ì‚¬ì", "ë¬´ì§", "ì£¼ë¶€", "ê°€ì •ì£¼ë¶€"],
            "íœ´ëŒ€í’ˆ": ["ì†Œì§€í’ˆ", "ì§", "ë¬¼ê±´", "ê°€ë°©"],
            "íŒŒì†": ["ì†ìƒ", "ì†í•´", "ê¹¨ì§", "ë§ê°€ì§"],
            
            # [í™•ì¥] ë³´í—˜ ì•½ê´€ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” í•µì‹¬ ë‹¨ì–´ë“¤
            "ìë™ì°¨": ["ì°¨", "ì°¨ëŸ‰", "ì œ ì°¨", "ë‚´ ì°¨", "ìê°€ìš©", "ìŠ¹ìš©ì°¨"],
            "ì‚¬ê³ ": ["ë¶€ë”ªí˜", "ì¶©ëŒ", "ë°•ìŒ", "ì ‘ì´‰ì‚¬ê³ ", "ê½", "ì‚¬ê±´"],
            "í”¼ë³´í—˜ì": ["ê°€ì…ì", "ë³´í—˜ ë“  ì‚¬ëŒ", "ê³„ì•½ì", "ë³´í—˜ ê°€ì…í•œ ì‚¬ëŒ"],
            "ë³´í—˜ì": ["ë³´í—˜íšŒì‚¬", "íšŒì‚¬", "ë³´í—˜ì‚¬", "ë³´í—˜ ì—…ì²´"],
            "ì•½ê´€": ["ê³„ì•½ì„œ", "ì„¤ëª…ì„œ", "ê·œì •ì§‘", "ë³´í—˜ ì±…ì", "ì¡°ê±´"],
            "ë³´í—˜ê¸ˆ": ["ë³´ìƒê¸ˆ", "ëˆ", "ì¹˜ë£Œë¹„", "ìˆ˜ë¦¬ë¹„", "ì§€ê¸‰ê¸ˆ"],
            "ë³´ìƒ": ["ë°°ìƒ", "ì§€ê¸‰", "ëˆ ë°›ê¸°", "ë³´ìƒë°›ê¸°"],
            "ì§€ê¸‰": ["ë°›ê¸°", "ì§€ê¸‰ë°›ê¸°", "ëˆ ë°›ê¸°", "ì§€ê¸‰í•´ì£¼ê¸°"],
            "ì†í•´": ["í”¼í•´", "ë§ê°€ì§", "ì†ìƒ", "ë¶€ìƒ"],
            "ê³„ì•½": ["ê°€ì…", "ë“  ê±°", "ì•½ì •", "ê³„ì•½í•˜ê¸°"],
            "ê°€ì…": ["ë“¤ê¸°", "ê³„ì•½", "ì‹ ì²­", "ë³´í—˜ ë“¤ê¸°"],
            "ê¸°ê°„": ["ì–¸ì œê¹Œì§€", "ë‚ ì§œ", "ìœ íš¨ê¸°ê°„", "ê¸°í•œ"],
            "í•´ì§€": ["ì·¨ì†Œ", "ê·¸ë§Œë‘ ", "íƒˆí‡´", "í™˜ë¶ˆ", "í•´ì•½"],
            "ìš´ì „": ["ë“œë¼ì´ë¸Œ", "ì£¼í–‰", "ëª°ë‹¤", "ìš´ì „í•˜ë‹¤"],
            "ë©´í—ˆ": ["ë¼ì´ì„ ìŠ¤", "ìš´ì „ì¦", "ìê²©ì¦", "ë©´í—ˆì¦"],
            "ì²­êµ¬": ["ì‹ ê³ ", "ì ‘ìˆ˜", "ìš”ì²­", "ì‹ ì²­"],
            "ìˆ˜ë¦¬": ["ê³ ì¹˜ê¸°", "ìˆ˜ë¦¬ë°›ê¸°", "ë³´ìˆ˜", "ìˆ˜ì„ "],
            "êµì²´": ["ë°”ê¾¸ê¸°", "êµí™˜", "ìƒˆê²ƒìœ¼ë¡œ ë°”ê¾¸ê¸°"],
            "ë„ë‚œ": ["í›”ì³ê°", "ë„ë‘‘", "ì—†ì–´ì§", "ë„ë‘‘ë§ìŒ"],
            "ì¹¨ìˆ˜": ["ë¬¼ì— ì ê¹€", "í™ìˆ˜", "ë¬¼ë°”ë‹¤", "ì¹¨ìˆ˜ë¨"],
            "í™”ì¬": ["ë¶ˆ", "ë¶ˆë‚¨", "ì „ì†Œ", "ë¶ˆíƒ€ë²„ë¦¼"],
            "í•œë„": ["ìµœëŒ€ ê¸ˆì•¡", "í•œê³„", "ìƒí•œì„ ", "ìµœëŒ€ í•œë„"],
            "ê³µì œ": ["ë¹¼ê¸°", "ì°¨ê°", "ê³µì œì•¡", "ë¹¼ëŠ” ê¸ˆì•¡"],
            "ìë…€": ["ì•„ë“¤", "ë”¸", "ì• ê¸°", "ìì‹", "ìì‹ë“¤"],
            "ê°€ì¡±": ["ì‹êµ¬", "ì• ë“¤", "ë¶€ëª¨ë‹˜", "ê°€ì¡±ë“¤"],
            "ì˜ì‚¬": ["ì„ ìƒë‹˜", "ì£¼ì¹˜ì˜", "ì „ë¬¸ì˜", "ì˜ë£Œì§„"],
            "ë³‘ì›": ["ì‘ê¸‰ì‹¤", "ì˜ì›", "ëŒ€í•™ë³‘ì›", "ì¹˜ë£Œë°›ì€ ê³³"],
            "ë¶€í’ˆ": ["íŒŒì¸ ", "ë¶€ì†", "ë¶€ë¶„", "ê¸°ê³„ ë¶€í’ˆ"],
            "íƒ€ì´ì–´": ["ë°”í€´", "íœ ", "íƒ€ì´ì–´"],
            "ë°°í„°ë¦¬": ["ë°§ë°ë¦¬", "ì¶©ì „ì§€", "ì „ì§€"]
        }
    
    # ì²­í¬ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
    found_keywords = []
    for key, synonyms in synonym_dict.items():
        if key in chunk_text:
            found_keywords.append((key, synonyms))
    
    if not found_keywords:
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê¸ì • QA ìƒì„±
        return generate_positive_qa(chunk_text, api_key)
    
    # ì²« ë²ˆì§¸ ë°œê²¬ëœ í‚¤ì›Œë“œì˜ ë™ì˜ì–´ë¡œ ì§ˆë¬¸ ë³€í˜•
    keyword, synonyms = found_keywords[0]
    synonym = random.choice(synonyms)
    
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì¼ë°˜ ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

**ì¤‘ìš”:** ì§ˆë¬¸ì—ì„œ "{keyword}" ëŒ€ì‹  "{synonym}" ê°™ì€ ë™ì˜ì–´ë‚˜ ìœ ì‚¬ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”. í•˜ì§€ë§Œ ë‹µë³€ì€ ì•½ê´€ì˜ ì›ë˜ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

[ë³´í—˜ ì•½ê´€]
{chunk_text[:2000]}

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: "{synonym}" ê°™ì€ ë™ì˜ì–´ë¥¼ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸
2. ë‹µë³€: ì•½ê´€ì˜ ì›ë˜ ìš©ì–´("{keyword}")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë‹µë³€
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš© - ë™ì˜ì–´ ì‚¬ìš©]
ë‹µë³€: [ì•½ê´€ì„ ì°¸ì¡°í•œ ë‹µë³€ë‚´ìš© - ì›ë˜ ìš©ì–´ ì‚¬ìš©]

### ìƒì„± ê²°ê³¼
"""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 800
        }
    }
    
    output = call_gemini_api(api_key, payload)
    if not output:
        return None
    
    # íŒŒì‹±
    lines = output.strip().split('\n')
    question = None
    answer = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
            question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
        elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
            answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
            answer_lines = [answer]
            for j in range(i + 1, len(lines)):
                next_line = lines[j].strip()
                if next_line and not next_line.startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                    answer_lines.append(next_line)
                else:
                    break
            answer = '\n'.join(answer_lines)
            break
    
    if question and answer:
        return {
            "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
            "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
            "output": answer,
            "type": "synonym"
        }
    
    return None


def load_and_classify_chunks(jsonl_path: str) -> dict:
    """ì²­í¬ë¥¼ ë¡œë“œí•˜ê³  ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
    chunks = {
        "negative": [],
        "positive": [],
        "calculation": [],
        "article": []
    }
    
    print("Loading and classifying chunks...")
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                item = json.loads(line)
                chunk_text = item.get('text') or item.get('chunk') or item.get('content', '')
                if chunk_text and len(chunk_text) > 100:
                    chunk_type = classify_chunk(chunk_text)
                    if chunk_type in chunks:
                        chunks[chunk_type].append(chunk_text)
            except json.JSONDecodeError:
                continue
    
    print(f"Classified chunks:")
    print(f"  - Negative: {len(chunks['negative'])}")
    print(f"  - Positive: {len(chunks['positive'])}")
    print(f"  - Calculation: {len(chunks['calculation'])}")
    print(f"  - Article: {len(chunks['article'])}")
    
    return chunks

def load_existing_data(file_path: str) -> list:
    """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… ê¸°ì¡´ ë°ì´í„° {len(data)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                return data
        except json.JSONDecodeError:
            print("âš ï¸ ê¸°ì¡´ íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return []
    return []

def main():
    parser = argparse.ArgumentParser(description="Generate v4 dataset with 3,800 samples")
    parser.add_argument(
        "--output",
        default=OUTPUT_PATH,
        help="Output JSON file path"
    )
    parser.add_argument(
        "--only-synonym",
        action="store_true",
        help="Generate only synonym data (skip other types)"
    )
    args = parser.parse_args()

    # 1. API í‚¤ ë¡œë“œ
    print("Loading API key...")
    api_key = load_api_key()
    print("âœ… API Key loaded")
    print(f"ğŸ“Œ Using model: {GEMINI_MODEL}")

    # 2. ì²­í¬ ë¡œë“œ ë° ë¶„ë¥˜
    chunks = load_and_classify_chunks(DATA_PATH)
    
    # 3. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° ì§„í–‰ ìƒí™© íŒŒì•…
    dataset = load_existing_data(args.output)
    stats = defaultdict(int)
    for item in dataset:
        stats[item.get('type', 'unknown')] += 1
    
    print(f"\nğŸ“Š Current Status (Loaded):")
    for k, v in stats.items():
        print(f"   - {k}: {v}")
    
    # 4. ë‚¨ì€ ìˆ˜ëŸ‰ ê³„ì‚°
    if args.only_synonym:
        # Synonymë§Œ ì¬ìƒì„±
        remaining_targets = {
            "negative": 0,
            "positive": 0,
            "calculation": 0,
            "article": 0,
            "synonym": max(0, TARGET_SYNONYM - stats["synonym"]),
        }
        print(f"\nğŸ”„ Synonymë§Œ ì¬ìƒì„± ëª¨ë“œ: {remaining_targets['synonym']}ê°œ í•„ìš”")
    else:
        remaining_targets = {
            "negative": max(0, TARGET_NEGATIVE - stats["negative"]),
            "positive": max(0, TARGET_POSITIVE - stats["positive"]),
            "calculation": max(0, TARGET_CALCULATION - stats["calculation"]),
            "article": max(0, TARGET_ARTICLE - stats["article"]),
            "synonym": max(0, TARGET_SYNONYM - stats["synonym"]),
        }

    # 5. ìƒ˜í”Œë§ (ë¶€ì¡±í•œ ë§Œí¼ë§Œ)
    sampled_chunks = {}
    
    # Negative
    if remaining_targets["negative"] > 0:
        available = chunks["negative"]
        count = min(remaining_targets["negative"], len(available))
        sampled_chunks["negative"] = random.sample(available, count)
    else:
        sampled_chunks["negative"] = []

    # Positive
    if remaining_targets["positive"] > 0:
        available = chunks["positive"]
        count = min(remaining_targets["positive"], len(available))
        sampled_chunks["positive"] = random.sample(available, count)
    else:
        sampled_chunks["positive"] = []

    # Calculation
    if remaining_targets["calculation"] > 0:
        available = chunks["calculation"]
        count = min(remaining_targets["calculation"], len(available))
        sampled_chunks["calculation"] = random.sample(available, count)
    else:
        sampled_chunks["calculation"] = []

    # Article
    if remaining_targets["article"] > 0:
        available = chunks["article"]
        count = min(remaining_targets["article"], len(available))
        sampled_chunks["article"] = random.sample(available, count)
    else:
        sampled_chunks["article"] = []
    
    # Synonym (Positiveì—ì„œ ìƒ˜í”Œë§)
    if remaining_targets["synonym"] > 0:
        available = chunks["positive"]
        count = min(remaining_targets["synonym"], len(available))
        sampled_chunks["synonym"] = random.sample(available, count)
    else:
        sampled_chunks["synonym"] = []

    
    print(f"\nğŸš€ Generation Plan (Remaining):")
    print(f"   - Negative: {len(sampled_chunks['negative'])} chunks")
    print(f"   - Positive: {len(sampled_chunks['positive'])} chunks")
    print(f"   - Calculation: {len(sampled_chunks['calculation'])} chunks")
    print(f"   - Article: {len(sampled_chunks['article'])} chunks")
    print(f"   - Synonym: {len(sampled_chunks['synonym'])} chunks")
    
    # 6. ì¶”ê°€ ë°ì´í„° ìƒì„±
    
    # 6-1. ë¶€ì • ì‚¬ë¡€ ìƒì„± (--only-synonym ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ)
    if not args.only_synonym and sampled_chunks['negative']:
        print(f"\nğŸ”´ Generating {len(sampled_chunks['negative'])} negative QA pairs...")
        for chunk in tqdm(sampled_chunks['negative'], desc="Negative"):
            qa_pair = generate_negative_qa(chunk, api_key)
            if qa_pair:
                dataset.append(qa_pair)
                stats['negative'] += 1
            
            if len(dataset) % 50 == 0:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(dataset)}ê°œ (Negative ì™„ë£Œ: {stats['negative']})")
            time.sleep(0.1) # ì†ë„ ë¹¨ë¼ì¡Œìœ¼ë¯€ë¡œ ë”œë ˆì´ ê°ì†Œ

    # 6-2. ê¸ì • ì‚¬ë¡€ ìƒì„± (--only-synonym ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ)
    if not args.only_synonym and sampled_chunks['positive']:
        print(f"\nğŸŸ¢ Generating {len(sampled_chunks['positive'])} positive QA pairs...")
        for chunk in tqdm(sampled_chunks['positive'], desc="Positive"):
            qa_pair = generate_positive_qa(chunk, api_key)
            if qa_pair:
                dataset.append(qa_pair)
                stats['positive'] += 1
            
            if len(dataset) % 50 == 0:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(dataset)}ê°œ (Positive ì™„ë£Œ: {stats['positive']})")
            time.sleep(0.1)

    # 6-3. ê³„ì‚° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (--only-synonym ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ)
    if not args.only_synonym and sampled_chunks['calculation']:
        print(f"\nğŸ”¢ Generating {len(sampled_chunks['calculation'])} calculation QA pairs...")
        for chunk in tqdm(sampled_chunks['calculation'], desc="Calculation"):
            qa_pair = generate_calculation_qa(chunk, api_key)
            if qa_pair:
                dataset.append(qa_pair)
                stats['calculation'] += 1
            
            if len(dataset) % 50 == 0:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(dataset)}ê°œ (Calculation ì™„ë£Œ: {stats['calculation']})")
            time.sleep(0.1)

    # 6-4. ì¡°í•­ ë²ˆí˜¸ ëª…ì‹œ ìƒì„± (--only-synonym ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ)
    if not args.only_synonym and sampled_chunks['article']:
        print(f"\nğŸ“‹ Generating {len(sampled_chunks['article'])} article QA pairs...")
        for chunk in tqdm(sampled_chunks['article'], desc="Article"):
            qa_pair = generate_article_qa(chunk, api_key)
            if qa_pair:
                dataset.append(qa_pair)
                stats['article'] += 1
            
            if len(dataset) % 50 == 0:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(dataset)}ê°œ (Article ì™„ë£Œ: {stats['article']})")
            time.sleep(0.1)

    # 6-5. ë™ì˜ì–´/ê²€ìƒ‰ ì‹¤íŒ¨ ëŒ€ì‘ ìƒì„±
    if sampled_chunks['synonym']:
        print(f"\nğŸ”„ Generating {len(sampled_chunks['synonym'])} synonym QA pairs...")
        print(f"â³ ì²œì²œíˆ ì§„í–‰ ì¤‘... (API í˜¸ì¶œ ê°„ê²©: 0.5ì´ˆ)")
        for chunk in tqdm(sampled_chunks['synonym'], desc="Synonym"):
            qa_pair = generate_synonym_qa(chunk, api_key)
            if qa_pair:
                dataset.append(qa_pair)
                stats['synonym'] += 1
            else:
                print(f"âš ï¸ Synonym ìƒì„± ì‹¤íŒ¨ (ì²­í¬ ìŠ¤í‚µ)")
            
            # ì¤‘ê°„ ì €ì¥ (ë” ìì£¼ ì €ì¥)
            if len(dataset) % 25 == 0:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {len(dataset)}ê°œ (Synonym ì™„ë£Œ: {stats['synonym']}/{len(sampled_chunks['synonym'])})")
            time.sleep(0.5)  # ì²œì²œíˆ ì§„í–‰ (0.1ì´ˆ -> 0.5ì´ˆ)

    # 7. ìµœì¢… ì €ì¥
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    # 8. í†µê³„ ì¶œë ¥
    print(f"\nâœ… Dataset saved to {args.output}")
    print(f"\nğŸ“Š Final Statistics:")
    print(f"   - Total: {len(dataset)} QA pairs")
    for type_name, count in stats.items():
        print(f"   - {type_name.capitalize()}: {count} ({count/len(dataset)*100:.1f}%)")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“ Sample (Negative):")
    negative_samples = [item for item in dataset if item.get('type') == 'negative']
    if negative_samples:
        print(json.dumps(negative_samples[0], ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()


