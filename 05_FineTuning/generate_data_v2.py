"""
íŒŒì¸íŠœë‹ìš© QA ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (RAG ìŠ¤íƒ€ì¼ - ê°œì„  ë²„ì „)
ì•½ê´€ ì²­í¬ë¥¼ Contextë¡œ í¬í•¨í•˜ì—¬ "ë¬¸ì„œë¥¼ ë³´ê³  ë‹µë³€í•˜ëŠ”" í•™ìŠµ ë°ì´í„° ìƒì„±
"""

import json
import random
import argparse
from pathlib import Path
from tqdm import tqdm
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline
)

# ì„¤ì •
DATA_PATH = "/home/pencilfoxs/0_Insurance_PF/01_Preprocessing/chunked_data.jsonl"
OUTPUT_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/train_dataset_rag.json"


def load_4bit_model(model_name: str):
    """4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ (ë°ì´í„° ìƒì„±ìš©)"""
    print(f"Loading data generation model: {model_name}...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=400,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        return_full_text=False
    )
    
    return pipe


def generate_qa_pair(chunk_text: str, pipe, max_retries: int = 3) -> dict | None:
    """
    ì•½ê´€ ì²­í¬ë¥¼ Contextë¡œ í¬í•¨í•˜ì—¬ RAG ìŠ¤íƒ€ì¼ì˜ QA ìŒ ìƒì„±
    
    Returns:
        {"instruction": ..., "input": ..., "output": ...} í˜•íƒœì˜ dict ë˜ëŠ” None
    """
    # RAG ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸: ì•½ê´€ì„ ë³´ê³  ë‹µë³€í•˜ëŠ” íŒ¨í„´ í•™ìŠµ
    prompt = f"""### ì§€ì‹œ
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ë³´í—˜ ì•½ê´€] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì¼ë°˜ ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ìš”êµ¬ì‚¬í•­]
1. ì§ˆë¬¸: ê³ ê°ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ (ì˜ˆ: "ì´ê±° ë³´ìƒ ë˜ë‚˜ìš”?", "ì¹¨ìˆ˜ëëŠ”ë° ì–´ë–¡í•˜ì£ ?")
2. ë‹µë³€: ë°˜ë“œì‹œ ì œê³µëœ ì•½ê´€ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€. ì•½ê´€ì— ëª…ì‹œëœ ì¡°í•­ì´ë‚˜ ê·¼ê±°ë¥¼ ì–¸ê¸‰í•˜ë©° ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª….
3. í˜•ì‹: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
ì§ˆë¬¸: [ì§ˆë¬¸ë‚´ìš©]
ë‹µë³€: [ì•½ê´€ì„ ì°¸ì¡°í•œ ë‹µë³€ë‚´ìš©]

### ë³´í—˜ ì•½ê´€ ë‚´ìš©
{chunk_text[:1500]}

### ìƒì„± ê²°ê³¼
"""
    
    for attempt in range(max_retries):
        try:
            output = pipe(prompt)[0]['generated_text']
            
            # íŒŒì‹± (ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ)
            lines = output.strip().split('\n')
            question = None
            answer = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('ì§ˆë¬¸:') or line.startswith('ì§ˆë¬¸ :'):
                    question = line.replace('ì§ˆë¬¸:', '').replace('ì§ˆë¬¸ :', '').strip()
                elif line.startswith('ë‹µë³€:') or line.startswith('ë‹µë³€ :'):
                    answer = line.replace('ë‹µë³€:', '').replace('ë‹µë³€ :', '').strip()
                    # ë‹µë³€ì€ ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´í›„ ì¤„ë“¤ë„ í¬í•¨
                    answer_lines = [answer]
                    idx = lines.index(line) + 1
                    while idx < len(lines) and not lines[idx].strip().startswith(('ì§ˆë¬¸:', 'ë‹µë³€:', '###')):
                        if lines[idx].strip():
                            answer_lines.append(lines[idx].strip())
                        idx += 1
                    answer = '\n'.join(answer_lines)
                    break
            
            if question and answer:
                # RAG ìŠ¤íƒ€ì¼ Instruction Tuning í¬ë§·
                # instruction: ëª¨ë¸ì˜ ì—­í• ê³¼ í–‰ë™ ì§€ì¹¨
                # input: ì°¸ì¡°í•  ì•½ê´€ ë¬¸ì„œ(Context) + ì§ˆë¬¸
                # output: ì•½ê´€ì„ ì°¸ì¡°í•œ ë‹µë³€
                return {
                    "instruction": "ì•„ë˜ ì œê³µëœ [ë³´í—˜ì•½ê´€]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ëª…ì‹œëœ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•˜ë©°, ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì•½ê´€ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.",
                    "input": f"[ë³´í—˜ì•½ê´€]\n{chunk_text[:1500]}\n\nì§ˆë¬¸: {question}",
                    "output": answer
                }
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            continue
    
    return None


def load_chunks(jsonl_path: str, max_chunks: int = 500) -> list:
    """JSONL íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ (ëœë¤ ìƒ˜í”Œë§)"""
    chunks = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                item = json.loads(line)
                # 'text' í•„ë“œê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‹¤ë¥¸ í•„ë“œ í™•ì¸
                chunk_text = item.get('text') or item.get('chunk') or item.get('content', '')
                if chunk_text and len(chunk_text) > 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    chunks.append(chunk_text)
            except json.JSONDecodeError:
                continue
    
    # ëœë¤ ìƒ˜í”Œë§
    if len(chunks) > max_chunks:
        chunks = random.sample(chunks, max_chunks)
    
    return chunks


def format_for_training(data: list) -> list:
    """
    Instruction Tuning í¬ë§·ìœ¼ë¡œ ë³€í™˜ (RAG ìŠ¤íƒ€ì¼)
    ìµœì¢… í˜•íƒœ: "### ì§€ì‹œ\n{instruction}\n### ì…ë ¥\n{input}\n### ì¶œë ¥\n{output}<|end_of_text|>"
    EOS í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜ë³µ ìƒì„± ë¬¸ì œ í•´ê²°
    """
    formatted = []
    for item in data:
        # EOS í† í° ëª…ì‹œì  ì¶”ê°€
        text = f"### ì§€ì‹œ\n{item['instruction']}\n### ì…ë ¥\n{item['input']}\n### ì¶œë ¥\n{item['output']}<|end_of_text|>"
        formatted.append({"text": text})
    return formatted


def main():
    parser = argparse.ArgumentParser(description="Generate RAG-style QA dataset for fine-tuning")
    parser.add_argument(
        "--model",
        default="beomi/Llama-3-Open-Ko-8B",
        help="Model for data generation"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=500,
        help="Number of QA pairs to generate"
    )
    parser.add_argument(
        "--output",
        default=OUTPUT_PATH,
        help="Output JSON file path"
    )
    args = parser.parse_args()

    # 1. ì²­í¬ ë¡œë“œ
    print(f"Loading chunks from {DATA_PATH}...")
    chunks = load_chunks(DATA_PATH, max_chunks=args.num_samples * 2)  # ì—¬ìœ ìˆê²Œ ë¡œë“œ
    print(f"Loaded {len(chunks)} chunks")

    # 2. ëª¨ë¸ ë¡œë“œ
    pipe = load_4bit_model(args.model)

    # 3. QA ìŒ ìƒì„± (RAG ìŠ¤íƒ€ì¼)
    print(f"\nGenerating {args.num_samples} RAG-style QA pairs...")
    print("ğŸ’¡ ê°œì„  ì‚¬í•­: ì•½ê´€ Contextë¥¼ í¬í•¨í•˜ì—¬ 'ë¬¸ì„œë¥¼ ë³´ê³  ë‹µë³€í•˜ëŠ”' íŒ¨í„´ í•™ìŠµ")
    dataset = []
    
    for i, chunk in enumerate(tqdm(chunks[:args.num_samples], desc="Generating")):
        qa_pair = generate_qa_pair(chunk, pipe)
        if qa_pair:
            dataset.append(qa_pair)
        
        # ì¤‘ê°„ ì €ì¥ (50ê°œë§ˆë‹¤)
        if (i + 1) % 50 == 0:
            formatted_dataset = format_for_training(dataset)
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(formatted_dataset, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(dataset)}ê°œ ìƒì„±ë¨")
    
    print(f"\nGenerated {len(dataset)} valid QA pairs")

    # 4. Instruction Tuning í¬ë§·ìœ¼ë¡œ ë³€í™˜ (EOS í† í° í¬í•¨)
    formatted_dataset = format_for_training(dataset)

    # 5. ì €ì¥
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(formatted_dataset, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Dataset saved to {output_path}")
    print(f"Total samples: {len(formatted_dataset)}")
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
    print(json.dumps(formatted_dataset[0], ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

