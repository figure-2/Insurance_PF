import json
import torch
import os
from tqdm import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import PeftModel

# 1. ì„¤ì • (Configuration)
BASE_MODEL_ID = "beomi/Llama-3-Open-Ko-8B"
ADAPTER_MODEL_DIR = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/llama-3-ko-insurance-lora-v3"
TEST_DATA_PATH = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/test_20.json"
OUTPUT_FILE = "/home/pencilfoxs/0_Insurance_PF/05_FineTuning/evaluation_result_full.json"

# ì¶”ë¡  ì„¤ì •
BATCH_SIZE = 8       # A100(40GB) ê¸°ì¤€ 8~16 ì •ë„ë©´ ì•ˆì „í•˜ê³  ë¹ ë¦„
MAX_NEW_TOKENS = 512 # ìƒì„±í•  ë‹µë³€ì˜ ìµœëŒ€ ê¸¸ì´

def main():
    print(f"ğŸš€ [Start] Full Inference on Test Set ({TEST_DATA_PATH})")

    # 2. ë°ì´í„° ë¡œë“œ
    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    print(f"ğŸ“‚ Loaded {len(test_data)} samples.")

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ¤– Loading Model...")
    
    # 4-bit ì–‘ìí™” ì„¤ì • (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # Base Model ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # LoRA Adapter ë³‘í•© (Inference Mode)
    model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_DIR)
    model.eval() # í‰ê°€ ëª¨ë“œ ì „í™˜

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" # ìƒì„± ì‹œì—ëŠ” left paddingì´ êµ­ë£°

    print("âœ… Model Loaded Successfully!")

    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ë°°ì¹˜ ì²˜ë¦¬
    results = []
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìˆœíšŒ
    for i in tqdm(range(0, len(test_data), BATCH_SIZE), desc="Processing Batches"):
        batch_items = test_data[i : i + BATCH_SIZE]
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Chat Template ì ìš© ê°€ëŠ¥í•˜ì§€ë§Œ ì—¬ê¸°ì„  í•™ìŠµ í¬ë§· ìœ ì§€)
        prompts = []
        for item in batch_items:
            # í•™ìŠµ ë•Œì™€ ë™ì¼í•œ í¬ë§· ì‚¬ìš© (ë‹µë³€ ë¶€ë¶„ ì œì™¸)
            prompt = (
                f"### ì§€ì‹œ\n{item['instruction']}\n\n"
                f"### ì…ë ¥\n{item['input']}\n\n"
                f"### ì¶œë ¥\n" # ë‹µë³€ ìƒì„±ì„ ìœ ë„í•˜ëŠ” ëë¶€ë¶„
            )
            prompts.append(prompt)

        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=2048
        ).to("cuda")

        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.1,    # ì‚¬ì‹¤ì ì¸ ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )

        # ë””ì½”ë”© ë° ê²°ê³¼ ì €ì¥
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë¥¼ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        input_len = inputs.input_ids.shape[1]
        generated_tokens = outputs[:, input_len:]
        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        for j, pred in enumerate(decoded_preds):
            original_item = batch_items[j]
            result_item = {
                "type": original_item.get("type", "unknown"),
                "instruction": original_item["instruction"],
                "ground_truth": original_item["output"], # ì •ë‹µ
                "prediction": pred.strip()               # ëª¨ë¸ ì˜ˆì¸¡
            }
            results.append(result_item)

    # 5. ê²°ê³¼ ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ Inference Completed! Results saved to {OUTPUT_FILE}")
    
    # ìƒ˜í”Œ ì¶œë ¥ (í™•ì¸ìš©)
    print("\nğŸ” [Sample Result]")
    print(f"Q: {results[0]['instruction']}")
    print(f"A(Model): {results[0]['prediction']}")
    print(f"A(Truth): {results[0]['ground_truth']}")

if __name__ == "__main__":
    main()
